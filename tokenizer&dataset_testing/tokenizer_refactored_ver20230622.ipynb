{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XfRlesI9ekPi"
      },
      "source": [
        "TODO list in 'deine data collactor' part\n",
        "1. json 파일에서 image size (height, width) 추가로 저장해서  read_ocr_core_engine() 에서 사용하기\n",
        "2. Dataset() class __init()__ 에서 label 불어로는 것 공부하여 적용하기\n",
        "3. user_prompt RvlCdipDataset.__get_item__() 부분에서 어떻게 집어넣을 지 생각해보기"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb-9YdQ_Bc16"
      },
      "source": [
        "#import module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfJa2S6OBVU1",
        "outputId": "8ea04332-22c9-4ec8-b131-fd8e79615500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xml_to_dict in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Using cached JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xml_to_dict #https://github.com/xthehatterx/xml_to_dict\n",
        "!pip install sentencepiece\n",
        "!pip install konlpy\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yUGKpQyEBiXt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QQASXvTEBipZ"
      },
      "source": [
        "#own location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1BJzRYKBkP4",
        "outputId": "253e1e13-5423-43c1-ab0a-d54546271d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/산학_테스트\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ColabNotebooks/산학_테스트\n",
        "xml_sample_loc = 'xml_sample_20230519.csv'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJgovoVflQi"
      },
      "source": [
        "# pre-training tokenizer part (2023.06.21. revised)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "12wBJAggBwBt"
      },
      "source": [
        "##get 중복없는 keyword in xml sample file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtZiIGF8Bzxd"
      },
      "outputs": [],
      "source": [
        "xml_sample = pd.read_csv(xml_sample_loc)\n",
        "keyword = xml_sample['keyword']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI5hrU4SB5n-"
      },
      "outputs": [],
      "source": [
        "#get 중복 없는 keyword\n",
        "keyword_unique = []\n",
        "\n",
        "for i in range(len(keyword)):\n",
        "  keyword_unique+=keyword[i].split('|')\n",
        "\n",
        "keyword_unique = list(set(keyword_unique))\n",
        "\n",
        "print(len(keyword_unique) )\n",
        "print(keyword_unique[:20])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YY6uQxryCF0X"
      },
      "source": [
        "##xml to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jUZ5m6BCHFD"
      },
      "outputs": [],
      "source": [
        "#same with https://github.com/miridi-sanhak/model/blob/main/preprocessing.py\n",
        "\n",
        "import json\n",
        "import math\n",
        "from io import BytesIO\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xml_to_dict\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def process_bbox(XML_BBOX, IM_SIZE, SHEET_SIZE, angle, center):\n",
        "    RATIO = IM_SIZE[0] / SHEET_SIZE[0]\n",
        "    x1, y1, x2, y2 = map(float, XML_BBOX)\n",
        "    x1, y1, x2, y2 = (x1 * RATIO, y1 * RATIO, x2 * RATIO, y2 * RATIO)\n",
        "    center = (center[0] * RATIO, center[1] * RATIO)\n",
        "\n",
        "    if angle != 0:\n",
        "        angle = 360 - angle\n",
        "        angle = math.radians(angle)\n",
        "        # Calculate the center point of the bbox\n",
        "        center_x, center_y = center\n",
        "        # Calculate the distance from the center to each corner of the bbox\n",
        "        distance_x = (x1 - center_x)\n",
        "        distance_y = (y1 - center_y)\n",
        "        # Apply rotation to the distances\n",
        "        new_distance_x = distance_x * math.cos(angle) - distance_y * math.sin(angle)\n",
        "        new_distance_y = distance_x * math.sin(angle) + distance_y * math.cos(angle)\n",
        "        # Calculate the new corners after rotation\n",
        "        x1 = center_x + new_distance_x\n",
        "        y1 = center_y + new_distance_y\n",
        "        x2 = center_x - new_distance_x\n",
        "        y2 = center_y - new_distance_y\n",
        "\n",
        "    x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
        "\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def get_render_bbox(text):\n",
        "    if text['RenderPos'] == None:\n",
        "        return []\n",
        "    render_pos = json.loads(text['RenderPos'])\n",
        "    render_bbox = []\n",
        "\n",
        "    left, top, right, bottom = map(float, text['Position'].values())\n",
        "\n",
        "    for render in render_pos['c']:\n",
        "        x, a, w, y = map(float, [render['x'], render['a'], render['w'], render['y']])\n",
        "        left_ = left + x\n",
        "        right_ = left_ + w\n",
        "        bottom_ = top + y\n",
        "        top_ = bottom_ - a\n",
        "\n",
        "        render_bbox.append((left_, top_, right_, bottom_))\n",
        "\n",
        "    return render_bbox\n",
        "\n",
        "def get_bbox(render_bbox):\n",
        "    min_x = min([x[0] for x in render_bbox])\n",
        "    min_y = min([x[1] for x in render_bbox])\n",
        "    max_x = max([x[2] for x in render_bbox])\n",
        "    max_y = max([x[3] for x in render_bbox])\n",
        "\n",
        "    return min_x, min_y, max_x, max_y\n",
        "\n",
        "def process_xml_dict(xml_dict, thumbnail):\n",
        "    processed_json = {}\n",
        "    processed_json['form'] = []\n",
        "\n",
        "    SHEET_SIZE = tuple(map(int, xml_dict['SHEET']['SHEETSIZE'].values()))\n",
        "    IM_SIZE = thumbnail.size\n",
        "\n",
        "    # Process XML to json\n",
        "    for i, text in enumerate(xml_dict['SHEET']['TEXT']):\n",
        "        left, top, right, bottom = map(float, text['Position'].values())\n",
        "        center = ((left + right) / 2, (top + bottom) / 2)\n",
        "\n",
        "        render_bbox = get_render_bbox(text)\n",
        "        if len(render_bbox) == 0: continue\n",
        "\n",
        "        XML_BBOX = get_bbox(render_bbox)\n",
        "\n",
        "        t = text['Text']\n",
        "        x1, y1, x2, y2 = process_bbox(XML_BBOX, IM_SIZE, SHEET_SIZE, int(text['@Rotate']), center)\n",
        "\n",
        "        processed_json['form'].append({\n",
        "            \"text\": t,\n",
        "            \"box\": [x1, y1, x2, y2],\n",
        "            \"font_id\": int(text['Font']['@FamilyIdx']),\n",
        "            \"font_size\": float(text['Font']['@Size']),\n",
        "            \"style\": {\n",
        "                \"bold\": text['Font']['Style']['@Bold'] == 'true',\n",
        "                \"italic\": text['Font']['Style']['@Italic'] == 'true',\n",
        "                \"strikeout\": text['Font']['Style']['@Strikeout'] == 'true',\n",
        "                \"underline\": text['Font']['Style']['@Underline'] == 'true'\n",
        "            },\n",
        "            \"linespace\": float(text['Font']['@LineSpace']),\n",
        "            \"opacity\": float(text['@Opacity']),\n",
        "            \"rotate\": float(text['@Rotate']),\n",
        "            \"id\": i\n",
        "        })\n",
        "\n",
        "        processed_json['form'][-1]['words'] = []\n",
        "\n",
        "        render_pos = json.loads(text['RenderPos'])\n",
        "\n",
        "        for j, bbox in enumerate(render_bbox):\n",
        "            x1_, y1_, x2_, y2_ = process_bbox(bbox, IM_SIZE, SHEET_SIZE, int(text['@Rotate']), center)\n",
        "            color = render_pos['c'][j]['f']\n",
        "            color = color[4:-1]\n",
        "            color = list(map(int, color.split(\",\")))\n",
        "            processed_json['form'][-1]['words'].append({\n",
        "                \"text\": render_pos['c'][j]['t'],\n",
        "                \"box\": [x1_, y1_, x2_, y2_],\n",
        "                \"font_size\": float(render_pos['c'][j]['s']),\n",
        "                \"letter_spacing\": float(render_pos['c'][j]['ds']),\n",
        "                \"font_id\": int(render_pos['c'][j]['yd']),\n",
        "                \"color\": color\n",
        "            })\n",
        "\n",
        "    return processed_json\n",
        "\n",
        "def process_xml(sheet_url, thumbnail_url):\n",
        "    sample_thumbnail = Image.open(BytesIO(requests.get(thumbnail_url).content))\n",
        "    sample_xml = requests.get(sheet_url).content.decode(\"utf-8\")\n",
        "    sample_json = xml_to_dict.XMLtoDict().parse(sample_xml)\n",
        "\n",
        "    processed_json = process_xml_dict(sample_json, sample_thumbnail)\n",
        "\n",
        "    return processed_json\n",
        "\n",
        "def make_sample_json():\n",
        "    # Read sample CSV and download thumbnail, XML\n",
        "    df = pd.read_csv(xml_sample_loc)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "\n",
        "      sample_sheet = df.iloc[i]\n",
        "\n",
        "      try:\n",
        "        processed_json = process_xml(sample_sheet['sheet_url'], sample_sheet['thumbnail_url'])\n",
        "\n",
        "        filename = f\"data/processed_sample_{i}.json\"\n",
        "        with open(filename, \"w\") as file_:\n",
        "          json.dump(processed_json, file_, indent=4)\n",
        "      except:\n",
        "        print(f\"error occurred in df line {i}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK0ecmL5CTG-"
      },
      "outputs": [],
      "source": [
        "make_sample_json() #약 30분 소요. xml파일을 json으로 변환"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3poxvfRhCXI5"
      },
      "source": [
        "##get text line in xml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fw2PM9qCZNK",
        "outputId": "80a2e04b-33f6-422f-c4ff-f8dec7b52dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error in data/processed_sample_2060.json\n"
          ]
        }
      ],
      "source": [
        "#json파일에서 text들을 가져와 text_sum에 저장\n",
        "import json\n",
        "import os\n",
        "\n",
        "text_sum = []\n",
        "\n",
        "folder_path = \"data/\"  # 데이터 폴더 경로\n",
        "\n",
        "# 폴더 내의 파일 목록 가져오기\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith(\".json\"):  # .json 파일인 경우에만 처리\n",
        "        file_path = os.path.join(folder_path, file_name)  # 파일 경로 생성\n",
        "\n",
        "    try:\n",
        "      with open(file_path, \"r\") as file_:\n",
        "          data = json.load(file_)\n",
        "\n",
        "      for i in range(len(data['form'])):\n",
        "          if(data['form'][i]['text']!=None):\n",
        "            text_sum.append(data['form'][i]['text'])  # 요소 추가\n",
        "\n",
        "    except:\n",
        "      print(f\"error in {file_path}\")\n",
        "\n",
        "\n",
        "print(len(text_sum))\n",
        "print(text_sum[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "66ZWFovSCkOZ"
      },
      "source": [
        "##train custom tokenizer (based on ke-t5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuK8UyKICcRb"
      },
      "outputs": [],
      "source": [
        "train_tokenization_data = np.concatenate((text_sum,keyword_unique)) #text와 keyword 합쳐서 한번에 train\n",
        "\n",
        "print(len(train_tokenization_data))\n",
        "\n",
        "def get_training_corpus(): #개수가 많으므로 1000개씩 끊어서 return\n",
        "    return (\n",
        "        train_tokenization_data[i : i + 1000]\n",
        "        for i in range(0, len(train_tokenization_data), 1000)\n",
        "    )\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2tiMdZ3CpMp"
      },
      "outputs": [],
      "source": [
        "old_tokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR/ke-t5-base-ko\") #pre-trained된거 불러오기\n",
        "old_tokenizer.vocab_size #64100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc584Ff3CupF"
      },
      "outputs": [],
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 15000) #뒤 숫자는 우리 데이터에서 단어 개수\n",
        "tokenizer.vocab_size #14602\n",
        "tokenizer.save_pretrained(\"tokenizer_finetuned_ket5_by_xml_data\") #저장"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f5L7gXTnDM9K"
      },
      "source": [
        "##see tokenized result (추후 학습과 관련 x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmrA993oCycz"
      },
      "outputs": [],
      "source": [
        "#tokenized by finetuned-ke-t5\n",
        "#text_sum 안의 text들을 토큰화하여 text_token_list에 저장.\n",
        "\n",
        "text_token_list = np.array([],dtype=object)\n",
        "\n",
        "for idx , word in enumerate(train_tokenization_data):\n",
        "  input_word = word\n",
        "  try:\n",
        "    tokens = tokenizer.tokenize(input_word)\n",
        "    text_token_list = np.concatenate((text_token_list,tokens))\n",
        "  except:\n",
        "    print(f\"error occur in input_word {input_word}, idx = {idx}\")\n",
        "\n",
        "print(text_token_list[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zj2zkgtC1SE"
      },
      "outputs": [],
      "source": [
        "#keyword token과 text token들을 합쳐서 중복제거.\n",
        "\n",
        "token_list = np.concatenate((text_token_list,keyword_unique))\n",
        "\n",
        "token_list = list(set(token_list))\n",
        "\n",
        "print(len(token_list) ) #==14602\n",
        "\n",
        "print(token_list[:20])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TLTUiNUTDSsq"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LmJo1DsmEAev"
      },
      "source": [
        "##define custom tokenizer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R3v22ARiECUc"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5TokenizerFast, PreTrainedTokenizer, PreTrainedTokenizerBase\n",
        "\n",
        "import re\n",
        "import sentencepiece as spm\n",
        "\n",
        "# The special tokens of T5Tokenizer is hard-coded with <extra_id_{}>\n",
        "# Created another class UDOPTokenizer extending it to add special visual tokens like <loc_{}>, etc.\n",
        "\n",
        "#class UdopTokenizer(T5Tokenizer):\n",
        "class UdopTokenizer(AutoTokenizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        eos_token=\"</s>\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        extra_ids=100,\n",
        "        loc_extra_ids=501,\n",
        "        other_extra_ids=200,\n",
        "        additional_special_tokens=[],\n",
        "        sp_model_kwargs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # Add extra_ids to the special token list\n",
        "        if extra_ids > 0 and not \"<extra_id_0>\" in additional_special_tokens:\n",
        "            additional_special_tokens = [\"<extra_id_{}>\".format(i) for i in range(extra_ids)]\n",
        "            additional_special_tokens.extend([\"<extra_l_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "            additional_special_tokens.extend([\"</extra_l_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "            additional_special_tokens.extend([\"<extra_t_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "            additional_special_tokens.extend([\"</extra_t_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "\n",
        "        if loc_extra_ids > 0 and not \"<loc_0>\" in additional_special_tokens:\n",
        "            additional_special_tokens.extend([\"<loc_{}>\".format(i) for i in range(loc_extra_ids)])\n",
        "\n",
        "        if other_extra_ids > 0 and not \"<other_0>\" in additional_special_tokens:\n",
        "            additional_special_tokens.extend([\"<other_{}>\".format(i) for i in range(other_extra_ids)])\n",
        "        print(additional_special_tokens)\n",
        "        PreTrainedTokenizer.__init__(\n",
        "            self,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            pad_token=pad_token,\n",
        "            extra_ids=extra_ids,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n",
        "\n",
        "        self.vocab_file = vocab_file\n",
        "        self._extra_ids = extra_ids\n",
        "        self._loc_extra_ids = loc_extra_ids\n",
        "        self._other_extra_ids = other_extra_ids\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self.sp_model.get_piece_size() + self._extra_ids * 5 + self._loc_extra_ids + self._other_extra_ids\n",
        "\n",
        "    def get_vocab(self):\n",
        "        vocab = {self.convert_ids_to_tokens(\n",
        "            i): i for i in range(self.vocab_size)}\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "        if token.startswith(\"<extra_id_\"):\n",
        "            match = re.match(r\"<extra_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 4\n",
        "        elif token.startswith(\"<extra_l_id_\"):\n",
        "            match = re.match(r\"<extra_l_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 3\n",
        "        elif token.startswith(\"</extra_l_id_\"):\n",
        "            match = re.match(r\"</extra_l_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 2\n",
        "        elif token.startswith(\"<extra_t_id_\"):\n",
        "            match = re.match(r\"<extra_t_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids\n",
        "        elif token.startswith(\"</extra_t_id_\"):\n",
        "            match = re.match(r\"</extra_t_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids\n",
        "        elif token.startswith(\"<loc_\"):\n",
        "            match = re.match(r\"<loc_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids\n",
        "        elif token.startswith(\"<other_\"):\n",
        "            match = re.match(r\"<other_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1\n",
        "\n",
        "        return self.sp_model.piece_to_id(token)\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        if index < self.sp_model.get_piece_size():\n",
        "            token = self.sp_model.IdToPiece(index)\n",
        "        else:\n",
        "\n",
        "            if index > self.sp_model.get_piece_size() + self._extra_ids * 5 + self._loc_extra_ids - 1:\n",
        "                index_loc = self.vocab_size - 1 - index\n",
        "                token = f\"<other_{index_loc}>\"\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 5 - 1:\n",
        "                index_loc = self.vocab_size - self._other_extra_ids - 1 - index\n",
        "                token = f\"<loc_{index_loc}>\"\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 4 - 1:\n",
        "                token = \"</extra_t_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 3 - 1:\n",
        "                token = \"<extra_t_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 2 - 1:\n",
        "                token = \"</extra_l_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 2 - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids - 1:\n",
        "                token = \"<extra_l_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 3 - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() - 1:\n",
        "                token = \"<extra_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 4 - 1 - index)\n",
        "            else:\n",
        "                raise\n",
        "        return token"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r2IgTNFREGiV"
      },
      "source": [
        "##test tokenizer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W9kAEC08EFBE"
      },
      "outputs": [],
      "source": [
        "my_tokenizer = UdopTokenizer.from_pretrained(\"tokenizer_finetuned_ket5_by_xml_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mqDECyjoEJlK",
        "outputId": "0efb7dee-5661-40bf-cae8-c4b3d2d18405"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'참hereistory'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_tokenizer.vocab_size\n",
        "my_tokenizer.decode([4321,1231,1050])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1PuGSgsMGFl",
        "outputId": "81b89dfd-84bf-486c-d81f-8d0fbddf62c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁제목', ':', '▁할로윈', '▁파티', '▁참여', '자', '▁모집합니다', '!', '▁안녕', '하세요', '▁여러분', '!', '▁올해', '▁할로윈', '을', '▁기념', '하기', '▁위해', '▁우리', '는', '▁특별', '한', '▁할로윈', '▁파티', '를', '▁개', '최', '하려', '고', '▁합니다', '!', '▁파티', '는', '▁다양', '한', '▁활동', '과', '▁게임', ',', '▁맛있', '는', '▁음식', ',', '▁그리', '고', '▁', '엄', '청', '난', '▁재미', '로', '▁가득', '할', '▁예정', '입니다', '.', '▁이', '▁특별', '한', '▁이벤트', '에', '▁참여', '하', '실', '▁분들', '을', '▁모집', '하고', '자', '▁이', '▁글', '을', '▁올', '립니다', '.', '▁일', '시', ':', '▁10월', '▁3', '1일', '▁(', '토', '요일', ')', '▁', '오후', '▁7', '시', '부', '터', '▁장소', ':', '▁(', '장소', '▁명', '시', ')', '▁참', '가', '비', ':', '▁개인', '당', '▁10,000', '원', '▁할로윈', '▁파티', '에', '▁참여', '하시', '면', '▁아래', '와', '▁같은', '▁활동', '을', '▁즐', '길', '▁수', '▁있습니다', ':', '▁할로윈', '▁코스', '튬', '▁경', '연', '▁대회', ':', '▁여러분', '의', '▁창의력', '을', '▁발', '휘', '해', '▁가', '장', '▁멋진', '▁코스', '튬', '으로', '▁참', '가', '해', '보세요', '.', '▁최고', '의', '▁코스', '튬', '은', '▁상', '을', '▁받', '게', '▁됩니다', '!', '▁호', '러', '▁영화', '▁시청', ':', '▁공포', '와', '▁스', '릴', '을', '▁선', '사', '하는', '▁할로윈', '▁분위기', '의', '▁영화', '들', '을', '▁함께', '▁감', '상', '해', '보세요', '.', '▁', '트', '릭', '▁오', '어', '▁', '트', '릿', ':', '▁사', '탕', '과', '▁함께', '▁할로윈', '▁테마', '의', '▁', '트', '릭', '▁오', '어', '▁', '트', '릿', '▁게임', '을', '▁즐', '기', '며', '▁다', '른', '▁참', '가', '자들', '과', '▁즐거', '운', '▁시간을', '▁보내', '세요.', '▁유령', '▁이야기', '▁시간', ':', '▁무서운', '▁이야기', '를', '▁나', '누', '며', '▁유령', '의', '▁존재', '를', '▁느껴', '보세요', '.', '▁이', '▁파티', '는', '▁할로윈', '을', '▁축하', '하며', '▁친구들', '과', '▁함께', '▁재미', '있는', '▁시간을', '▁보내', '고', '▁싶은', '▁분들', '을', '▁위해', '▁준비', '되었', '습니다', '.', '▁모두', '가', '▁함께', '▁할로윈', '▁분위기', '를', '▁만들어', '나', '갈', '▁수', '▁있', '도', '록', '▁참여', '자', '▁여러분', '의', '▁', '활', '발', '한', '▁참여', '를', '▁기다', '립니다', '!', '▁참', '가', '▁신청', '은', '▁아래', '의', '▁연락처', '로', '▁신청', '해주', '시기', '▁바', '랍니다', ':', '▁이름', ':', '▁연락처', ':', '▁인', '원', '▁수', ':', '▁많', '은', '▁관심', '과', '▁참여', '▁부탁드립니다', '.', '▁함께', '▁무서운', '▁할로윈', '을', '▁즐', '기', '는', '▁파티', '에서', '▁만', '날', '▁수', '▁있', '기', '를', '▁기대', '합니다', '!', '▁감사합니다', '.', '▁[', '이름', ']', '▁[', '연', '락', '처', ']']\n",
            "[2612, 214, 6626, 7715, 2306, 505, 9373, 138, 8515, 2602, 5489, 138, 8361, 6626, 144, 2380, 758, 562, 1123, 181, 4012, 191, 6626, 7715, 185, 891, 13958, 7656, 353, 3309, 138, 7715, 181, 1526, 191, 1514, 309, 5258, 113, 10191, 181, 4232, 113, 4951, 353, 103, 3297, 2996, 2483, 4656, 300, 9259, 501, 5801, 304, 105, 382, 4012, 191, 2464, 212, 2306, 1518, 441, 3289, 144, 1727, 498, 505, 382, 4432, 144, 7463, 11818, 105, 751, 345, 214, 4759, 198, 2269, 219, 2225, 6117, 173, 103, 9452, 447, 345, 615, 523, 1730, 214, 219, 11086, 2737, 345, 173, 4321, 262, 715, 214, 1740, 2749, 3590, 614, 6626, 7715, 212, 2306, 5338, 580, 6757, 335, 3642, 1514, 144, 4872, 1330, 332, 756, 214, 6626, 6372, 2, 2268, 2473, 7936, 214, 5489, 168, 4555, 144, 1864, 6211, 389, 489, 347, 3928, 6372, 2, 311, 4321, 262, 389, 4819, 105, 3091, 168, 6372, 2, 200, 1014, 144, 2651, 461, 4487, 138, 3339, 1055, 4987, 9760, 214, 9052, 335, 2432, 6512, 144, 1585, 433, 349, 6626, 2547, 168, 4987, 650, 144, 983, 3566, 874, 389, 4819, 105, 103, 559, 6476, 1075, 495, 103, 559, 3059, 214, 1355, 6470, 309, 983, 6626, 8928, 168, 103, 559, 6476, 1075, 495, 103, 559, 3059, 5258, 144, 4872, 248, 1346, 895, 2102, 4321, 262, 7732, 309, 6713, 520, 7371, 9219, 1160, 9861, 3634, 1950, 214, 8651, 3634, 185, 677, 6112, 1346, 9861, 168, 6009, 185, 8432, 4819, 105, 382, 7715, 181, 6626, 144, 10167, 1776, 3496, 309, 983, 4656, 2234, 7371, 9219, 353, 3172, 3289, 144, 562, 1904, 5068, 924, 105, 3507, 262, 983, 6626, 2547, 185, 4068, 610, 3704, 332, 1775, 302, 1173, 2306, 505, 5489, 168, 103, 3575, 2475, 191, 2306, 185, 7035, 11818, 138, 4321, 262, 1147, 200, 6757, 168, 4615, 300, 1147, 10891, 3194, 1187, 3444, 214, 5303, 214, 4615, 214, 1249, 614, 332, 214, 1405, 200, 2113, 309, 2306, 4189, 105, 983, 8651, 6626, 144, 4872, 248, 181, 7715, 434, 2093, 1450, 332, 1775, 248, 185, 9445, 317, 138, 7321, 105, 683, 3201, 585, 683, 2473, 1947, 2107, 585]\n",
            "제목: 할로윈 파티 참여자 모집합니다! 안녕하세요 여러분! 올해 할로윈을 기념하기 위해 우리는 특별한 할로윈 파티를 개최하려고 합니다! 파티는 다양한 활동과 게임, 맛있는 음식, 그리고 엄청난 재미로 가득할 예정입니다. 이 특별한 이벤트에 참여하실 분들을 모집하고자 이 글을 올립니다. 일시: 10월 31일 (토요일) 오후 7시부터 장소: (장소 명시) 참가비: 개인당 10,000원 할로윈 파티에 참여하시면 아래와 같은 활동을 즐길 수 있습니다: 할로윈 코스<unk> 경연 대회: 여러분의 창의력을 발휘해 가장 멋진 코스<unk>으로 참가해보세요. 최고의 코스<unk>은 상을 받게 됩니다! 호러 영화 시청: 공포와 스릴을 선사하는 할로윈 분위기의 영화들을 함께 감상해보세요. 트릭 오어 트릿: 사탕과 함께 할로윈 테마의 트릭 오어 트릿 게임을 즐기며 다른 참가자들과 즐거운 시간을 보내세요. 유령 이야기 시간: 무서운 이야기를 나누며 유령의 존재를 느껴보세요. 이 파티는 할로윈을 축하하며 친구들과 함께 재미있는 시간을 보내고 싶은 분들을 위해 준비되었습니다. 모두가 함께 할로윈 분위기를 만들어나갈 수 있도록 참여자 여러분의 활발한 참여를 기다립니다! 참가 신청은 아래의 연락처로 신청해주시기 바랍니다: 이름: 연락처: 인원 수: 많은 관심과 참여 부탁드립니다. 함께 무서운 할로윈을 즐기는 파티에서 만날 수 있기를 기대합니다! 감사합니다. [이름] [연락처]\n"
          ]
        }
      ],
      "source": [
        "user_text = '''제목: 할로윈 파티 참여자 모집합니다!\n",
        "\n",
        "안녕하세요 여러분!\n",
        "\n",
        "올해 할로윈을 기념하기 위해 우리는 특별한 할로윈 파티를 개최하려고 합니다! 파티는 다양한 활동과 게임, 맛있는 음식, 그리고 엄청난 재미로 가득할 예정입니다. 이 특별한 이벤트에 참여하실 분들을 모집하고자 이 글을 올립니다.\n",
        "\n",
        "일시: 10월 31일 (토요일) 오후 7시부터\n",
        "장소: (장소 명시)\n",
        "참가비: 개인당 10,000원\n",
        "\n",
        "할로윈 파티에 참여하시면 아래와 같은 활동을 즐길 수 있습니다:\n",
        "\n",
        "할로윈 코스튬 경연 대회: 여러분의 창의력을 발휘해 가장 멋진 코스튬으로 참가해보세요. 최고의 코스튬은 상을 받게 됩니다!\n",
        "호러 영화 시청: 공포와 스릴을 선사하는 할로윈 분위기의 영화들을 함께 감상해보세요.\n",
        "트릭 오어 트릿: 사탕과 함께 할로윈 테마의 트릭 오어 트릿 게임을 즐기며 다른 참가자들과 즐거운 시간을 보내세요.\n",
        "유령 이야기 시간: 무서운 이야기를 나누며 유령의 존재를 느껴보세요.\n",
        "\n",
        "이 파티는 할로윈을 축하하며 친구들과 함께 재미있는 시간을 보내고 싶은 분들을 위해 준비되었습니다. 모두가 함께 할로윈 분위기를 만들어나갈 수 있도록 참여자 여러분의 활발한 참여를 기다립니다!\n",
        "\n",
        "참가 신청은 아래의 연락처로 신청해주시기 바랍니다:\n",
        "\n",
        "이름:\n",
        "연락처:\n",
        "인원 수:\n",
        "\n",
        "많은 관심과 참여 부탁드립니다. 함께 무서운 할로윈을 즐기는 파티에서 만날 수 있기를 기대합니다!\n",
        "\n",
        "감사합니다.\n",
        "\n",
        "[이름] [연락처]'''\n",
        "\n",
        "tokens = my_tokenizer.tokenize(user_text)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "ids = my_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)\n",
        "\n",
        "decoded = my_tokenizer.decode(ids)\n",
        "\n",
        "print(decoded)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MuEpv5Nlh5T2"
      },
      "source": [
        "#define data collactor (2023.06.22 revised)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fMEq4ONo6qKx"
      },
      "source": [
        "##잡다한 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp_APhcjuJwB"
      },
      "outputs": [],
      "source": [
        "########################## this block is for testing ############################\n",
        "import json\n",
        "import os\n",
        "\n",
        "folder_path = \"data/\"  # 데이터 폴더 경로\n",
        "\n",
        "# 폴더 내의 파일 목록 가져오기\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "file_name = file_list[14]\n",
        "\n",
        "print(file_name)\n",
        "\n",
        "if file_name.endswith(\".json\"):  # .json 파일인 경우에만 처리\n",
        "  file_path = os.path.join(folder_path, file_name)  # 파일 경로 생성\n",
        "\n",
        "  with open(file_path, \"r\") as file_:\n",
        "    data = json.load(file_)\n",
        "\n",
        "  print(data)\n",
        "  print()\n",
        "  print(data['form'])\n",
        "  print(len(data['form']))\n",
        "\n",
        "  for i in range(len(data['form'])):\n",
        "    if(data['form'][i]['text']!=None):\n",
        "      print(data['form'][i]['text'])\n",
        "      print(data['form'][i]['box'])\n",
        "      print(data['form'][i]['words']) #우선은 단어별이 아닌 문장별로 진행\n",
        "      print()\n",
        "\n",
        "  for form in data['form']:\n",
        "    for word in form['words']:\n",
        "      print(word)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5S3ZNoZQ6zNl"
      },
      "source": [
        "## 기본 util 함수들 (UDOP github 복붙내용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYgrnNhAsQj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "    codes for just preprocessing\n",
        "\"\"\"\n",
        "\n",
        "def normalText(t):\n",
        "    if type(t) is float:\n",
        "        if t == int(t):\n",
        "            t = int(t)\n",
        "    t = str(t)\n",
        "    return t.strip()\n",
        "\n",
        "\n",
        "def get_prop(node, name):\n",
        "    title = node.get(\"title\")\n",
        "    props = title.split(\";\")\n",
        "    for prop in props:\n",
        "        (key, args) = prop.split(None, 1)\n",
        "        args = args.strip('\"')\n",
        "        if key == name:\n",
        "            return args\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_bb(bb):\n",
        "    bbs = [float(j) for j in bb]\n",
        "    xs, ys = [], []\n",
        "    for i, b in enumerate(bbs):\n",
        "        if i % 2 == 0:\n",
        "            xs.append(b)\n",
        "        else:\n",
        "            ys.append(b)\n",
        "    return [min(xs), min(ys), max(xs), max(ys)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4OSf93pnzxho"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "\"\"\"\n",
        "    codes for just preprocessing image data\n",
        "\"\"\"\n",
        "def get_visual_bbox(image_size=224):\n",
        "    image_feature_pool_shape = [image_size//16, image_size//16]\n",
        "    visual_bbox_x = (torch.arange(\n",
        "        0,\n",
        "        1.0 * (image_feature_pool_shape[1] + 1),\n",
        "        1.0,\n",
        "    ) / image_feature_pool_shape[1])\n",
        "    visual_bbox_y = (torch.arange(\n",
        "        0,\n",
        "        1.0 * (image_feature_pool_shape[0] + 1),\n",
        "        1.0,\n",
        "    ) / image_feature_pool_shape[0])\n",
        "    visual_bbox_input = torch.stack(\n",
        "        [\n",
        "            visual_bbox_x[:-1].repeat(\n",
        "                image_feature_pool_shape[0], 1),\n",
        "            visual_bbox_y[:-1].repeat(\n",
        "                image_feature_pool_shape[1], 1).transpose(\n",
        "                    0, 1),\n",
        "            visual_bbox_x[1:].repeat(\n",
        "                image_feature_pool_shape[0], 1),\n",
        "            visual_bbox_y[1:].repeat(\n",
        "                image_feature_pool_shape[1], 1).transpose(\n",
        "                    0, 1),\n",
        "        ],\n",
        "        dim=-1,\n",
        "    ).view(-1, 4)\n",
        "    return visual_bbox_input\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std, format='rgb'):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.format = format.lower()\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if 'bgr' in self.format:\n",
        "            image = image[[2, 1, 0]]\n",
        "        if '255' in self.format:\n",
        "            image = image * 255\n",
        "        if image.size(0) == 1:\n",
        "            image = image.repeat(3, 1, 1)\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image\n",
        "\n",
        "def img_trans_torchvision(image, image_size=224):\n",
        "    trans = T.Compose([\n",
        "            T.Resize([image_size,image_size]),\n",
        "            T.ToTensor(),\n",
        "            Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )])\n",
        "\n",
        "    image = trans(image)  # copy to make it writeable\n",
        "    return image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EhwgfrlO64pu"
      },
      "source": [
        "##data collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LpmQ2hRXh82I"
      },
      "outputs": [],
      "source": [
        "class DataCollatorForT5DocCLS:\n",
        "    \"\"\"\n",
        "    Data collator used for T5 document classification\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer=None, meta_path=None, input_length=None, target_length=None, pad_token_id=None, decoder_start_token_id=None):\n",
        "\n",
        "        self.tokenizer = tokenizer #이전에 만든 udop tokenizer를 불러옴\n",
        "        self.input_length = input_length\n",
        "        self.target_length = target_length\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.decoder_start_token_id = decoder_start_token_id\n",
        "\n",
        "    def __call__(self, user_prompt ,ori_input_ids, ori_bbox_list, labels=None):\n",
        "\n",
        "        # \"원래 input text 정보 & bounding box\"\n",
        "        # -->\n",
        "        # \"prompt text 정보 + 원래 input text 정보\" list\n",
        "        # +\n",
        "        # [0,0,0,0]을 promt text token 개수만큼 + 원래 bounding box\n",
        "\n",
        "        #prompt_text = 'document classification.'\n",
        "        prompt_text = user_prompt\n",
        "        prompt_ids =  self.tokenizer.encode(prompt_text, add_special_tokens=False)\n",
        "        input_ids = prompt_ids + ori_input_ids\n",
        "        bbox_list = [[0,0,0,0]] * len(prompt_ids) + ori_bbox_list\n",
        "\n",
        "        if(labels!=None):  #label은 classification에서만 수행\n",
        "        #인줄 알았는데 layout modeling 이런것도 다 output이 있으니까 label==output 인건가..???\n",
        "          labels = self.tokenizer.encode(labels, add_special_tokens=True)\n",
        "\n",
        "        return input_ids, labels, bbox_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ENujqg_z68np"
      },
      "source": [
        "##dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BKcHFAituZb"
      },
      "outputs": [],
      "source": [
        "# 해당 부분은 json파일의 문장을 line-by-line으로 읽는 것으로 해당 함수 수정 완료.\n",
        "def read_ocr_core_engine(file, image_dir, tokenizer, max_seq_length=None, num_img_embeds=None, image_size=224):\n",
        "    #max_seq_length와 num_img_embeds 는 원본 코드에서도 안쓰는데 왜있는거지?\n",
        "\n",
        "    with open(file, 'r', encoding='utf8') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "        except:\n",
        "            data = {}\n",
        "    rets = []\n",
        "    n_split = 0\n",
        "\n",
        "    page_size = (1280,720) ############################################### 이거 바꿔야됌 #############\n",
        "    #page_size = (width, height) 추후 해당 내용 json파일 추가 필요\n",
        "    tiff_images = Image.open(image_dir)\n",
        "    image = img_trans_torchvision(tiff_images, image_size)\n",
        "\n",
        "    text_list, bbox_list = [], []\n",
        "    for form in data['form']: #문장별로 쪼갬\n",
        "      for word in form['words']: #단어별로 쪼갬\n",
        "\n",
        "        if word == ' ': #띄어쓰기는 건너뛰기\n",
        "          continue\n",
        "\n",
        "        sub_tokens = tokenizer.tokenize(word['text']) #단어별로 쪼갠걸 다시 토큰화 (하나의 단어도 여러개의 토큰 가능)\n",
        "        for sub_token in sub_tokens:\n",
        "          text_list.append(sub_token)\n",
        "          bbox_list.apend(word['box']) #현재는 단어별 bbox, 추후 문장별 bbox로도 수정 가능\n",
        "          #bbox_list.append(form['box'])\n",
        "\n",
        "    if len(text_list) > 0:\n",
        "      rets.append([text_list, bbox_list, image, page_size])\n",
        "\n",
        "    assert len(text_list) == len(bbox_list)\n",
        "    n_split = len(rets)\n",
        "\n",
        "    return rets, n_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXgUVgtL2svj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "EMPTY_BOX = [0, 0, 0, 0]\n",
        "SEP_BOX = [1000, 1000, 1000, 1000]\n",
        "\n",
        "class RvlCdipDataset(Dataset):\n",
        "\n",
        "    #NUM_LABELS = 16\n",
        "\n",
        "    def __init__(self , tokenizer , data_args , mode='train'):\n",
        "\n",
        "        \"\"\" Structure of data directory:\n",
        "\n",
        "            --- xml_sample_loc (.csv)\n",
        "                   ├── images_url\n",
        "                   └── labels_url\n",
        "            --- data (folder)\n",
        "                   └── processed_sample{index} .json\n",
        "        \"\"\"\n",
        "        self.main_df = pd.read_csv(xml_sample_loc) # xml_sample.csv 파일 저장\n",
        "\n",
        "        self.sheet_url = 'sheet_url'\n",
        "        self.image_url = 'thumbnail_url'\n",
        "\n",
        "        if mode == 'train': #train ,val, test 에 따라 사용하는 data의 범위가 다름. (근데 self-supervised도 이거 필요 있나..? )\n",
        "            file_data_range = ( 0 , int(len(self.main_df) * 0.6 ) )\n",
        "        elif mode == 'val':\n",
        "            file_data_range = ( int(len(self.main_df) * 0.6 ) , int(len(self.main_df) * 0.8 ) )\n",
        "        elif mode == 'test':\n",
        "            file_data_range = ( int(len(self.main_df) * 0.8 ) , int(len(self.main_df) ) )\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.cls_bbox = EMPTY_BOX[:]\n",
        "        self.pad_bbox = EMPTY_BOX[:]\n",
        "        self.sep_bbox = SEP_BOX[:]\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = data_args.max_seq_length\n",
        "        self.num_img_embeds = 0\n",
        "\n",
        "        label_list = None #get_rvlcdip_labels() #classification task에서만 사용하는 변수\n",
        "        self.label_list = label_list\n",
        "        self.label_map = dict(zip(list(range(len(self.label_list))), self.label_list))\n",
        "        self.n_classes = len(label_list)\n",
        "        self.label_list = label_list\n",
        "\n",
        "        self.image_size = data_args.image_size\n",
        "\n",
        "        self.examples = []\n",
        "        self.labels = []\n",
        "        self.images = []\n",
        "\n",
        "        self.cls_collator = DataCollatorForT5DocCLS( #기존에 정의한 토크나이저 선언\n",
        "                tokenizer=tokenizer,\n",
        "            )\n",
        "\n",
        "\n",
        "        results = [self.load_file(file_idx) for file_idx in tqdm(range(file_data_range[0],file_data_range[1]))]\n",
        "        for labels, examples, images in results:\n",
        "            self.labels += labels\n",
        "            self.examples += examples\n",
        "            self.images += images\n",
        "        assert len(self.labels) == len(self.examples)\n",
        "\n",
        "    def load_file(self, file_idx):\n",
        "        labels, examples, images = [], [], []\n",
        "\n",
        "        labels.append(0) ############### label 미정으로 일단 다 0 ##########\n",
        "        examples.append(f\"data/processed_sample_{file_idx} .json\")\n",
        "        images.append(self.main_df[file_idx][self.image_url])\n",
        "\n",
        "        return labels, examples, images\n",
        "\n",
        "    def __getitem__(self, index): #완료\n",
        "        try:\n",
        "            label = self.labels[index]\n",
        "            label = self.label_map[int(label)]\n",
        "\n",
        "            rets, n_split = read_ocr_core_engine(self.examples[index], self.images[index] , self.tokenizer, self.max_seq_length, self.num_img_embeds, self.image_size)\n",
        "\n",
        "            if n_split == 0:\n",
        "                # Something wrong with the .ocr.json file\n",
        "                print(f\"EMPTY ENTRY in index {index}\")\n",
        "                return self[(index + 1) % len(self)]\n",
        "            for i in range(n_split): #정상적으로 코드 실행됬다면 n_split==1 임.\n",
        "                text_list, bbox_list, image, page_size = rets[i]\n",
        "                (width, height) = page_size\n",
        "                bbox = [  #이미지 크기에 맞게 정규화\n",
        "                    [\n",
        "                        b[0] / width,\n",
        "                        b[1] / height,\n",
        "                        b[2] / width,\n",
        "                        b[3] / height,\n",
        "                    ]\n",
        "                    for b in bbox_list\n",
        "                ]\n",
        "\n",
        "                visual_bbox_input = get_visual_bbox(self.image_size) # (x_min, y_min, x_max, y_max) 형태의 좌표로 이루어진 텐서 반환\n",
        "\n",
        "                input_ids = self.tokenizer.convert_tokens_to_ids(text_list) #토큰 자른것들을 token id들로 변환\n",
        "\n",
        "                input_ids, labels, bbox_input = self.cls_collator(\"user prompt\", input_ids, bbox, label) #prompt 붙여서 최종 input,bbox,label을 만듦. ################################\n",
        "                attention_mask = [1] * len(input_ids)\n",
        "                decoder_attention_mask = [1] * len(labels)\n",
        "\n",
        "                char_list = [0]\n",
        "                char_bbox_list = [[0,0,0,0]]\n",
        "                char_ids = torch.tensor(char_list, dtype=torch.long)\n",
        "                char_bbox_input = torch.tensor(char_bbox_list, dtype=torch.float)\n",
        "\n",
        "                bbox_input = torch.tensor(bbox_input, dtype=torch.float)\n",
        "                labels = torch.tensor(labels, dtype=torch.long)\n",
        "                input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "                attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "                decoder_attention_mask = torch.tensor(decoder_attention_mask, dtype=torch.long)\n",
        "                assert len(bbox_input) == len(input_ids)\n",
        "                assert len(bbox_input.size()) == 2\n",
        "                assert len(char_bbox_input.size()) == 2\n",
        "\n",
        "                return_dict =  {\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"attention_mask\": attention_mask,\n",
        "                    \"labels\": labels,\n",
        "                    \"seg_data\": bbox_input,\n",
        "                    \"visual_seg_data\": visual_bbox_input,\n",
        "                    \"decoder_attention_mask\": decoder_attention_mask,\n",
        "                    \"image\": image,\n",
        "                    'char_ids': char_ids,\n",
        "                    'char_seg_data': char_bbox_input\n",
        "                }\n",
        "                assert input_ids is not None\n",
        "\n",
        "                return return_dict\n",
        "        except:\n",
        "            return self[(index + 1) % len(self)]\n",
        "\n",
        "    #def get_labels(self): # classification에서 label의 종류 출력하는 함수. 우리는 필요 없을 듯.\n",
        "    #    return list(map(str, list(range(self.NUM_LABELS))))\n",
        "\n",
        "    def pad_tokens(self, input_ids, bbox): #이건 그냥 길이 max_len에 맞게 맞추는 함수\n",
        "        # [CLS], sentence, [SEP]\n",
        "        tokenized_tokens = self.tokenizer.build_inputs_with_special_tokens(input_ids)\n",
        "        start_token, _, end_token = tokenized_tokens[0], tokenized_tokens[1:-1], tokenized_tokens[-1]\n",
        "\n",
        "        sentence = tokenized_tokens\n",
        "        expected_seq_length = self.max_seq_length - self.num_img_embeds\n",
        "        mask = torch.zeros(expected_seq_length)\n",
        "        mask[:len(sentence)] = 1\n",
        "\n",
        "        bbox = [self.cls_bbox] + bbox + [self.sep_bbox]\n",
        "        while len(sentence) < expected_seq_length:\n",
        "            sentence.append(self.tokenizer.pad_token_id)\n",
        "            bbox.append(self.pad_bbox)\n",
        "\n",
        "        assert len(sentence) == len(bbox)\n",
        "        return (sentence, mask, bbox, start_token, end_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QD8aQLANh_qF"
      },
      "outputs": [],
      "source": [
        "temp = DataCollatorForT5DocCLS(tokenizer = my_tokenizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zb-9YdQ_Bc16",
        "QQASXvTEBipZ",
        "12wBJAggBwBt",
        "YY6uQxryCF0X",
        "3poxvfRhCXI5",
        "66ZWFovSCkOZ",
        "f5L7gXTnDM9K",
        "LmJo1DsmEAev",
        "r2IgTNFREGiV",
        "fMEq4ONo6qKx",
        "5S3ZNoZQ6zNl"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
