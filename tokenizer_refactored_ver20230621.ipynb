{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zb-9YdQ_Bc16",
        "QQASXvTEBipZ",
        "12wBJAggBwBt",
        "YY6uQxryCF0X",
        "3poxvfRhCXI5",
        "66ZWFovSCkOZ",
        "f5L7gXTnDM9K",
        "LmJo1DsmEAev",
        "r2IgTNFREGiV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#import module"
      ],
      "metadata": {
        "id": "Zb-9YdQ_Bc16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfJa2S6OBVU1",
        "outputId": "8e8d7987-a72b-433a-fb3c-59a943ef3b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xml_to_dict\n",
            "  Downloading xml_to_dict-0.1.6-py3-none-any.whl (3.6 kB)\n",
            "Installing collected packages: xml_to_dict\n",
            "Successfully installed xml_to_dict-0.1.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xml_to_dict #https://github.com/xthehatterx/xml_to_dict\n",
        "!pip install sentencepiece\n",
        "!pip install konlpy\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "yUGKpQyEBiXt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#own location"
      ],
      "metadata": {
        "id": "QQASXvTEBipZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ColabNotebooks/산학_테스트\n",
        "xml_sample_loc = 'xml_sample_20230519.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1BJzRYKBkP4",
        "outputId": "7003f03a-d5e7-45a3-ffcb-02bcb385ae94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/산학_테스트\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get 중복없는 keyword in xml sample file"
      ],
      "metadata": {
        "id": "12wBJAggBwBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xml_sample = pd.read_csv(xml_sample_loc)\n",
        "keyword = xml_sample['keyword']"
      ],
      "metadata": {
        "id": "dtZiIGF8Bzxd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get 중복 없는 keyword\n",
        "keyword_unique = []\n",
        "\n",
        "for i in range(len(keyword)):\n",
        "  keyword_unique+=keyword[i].split('|')\n",
        "\n",
        "keyword_unique = list(set(keyword_unique))\n",
        "\n",
        "print(len(keyword_unique) )\n",
        "print(keyword_unique[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI5hrU4SB5n-",
        "outputId": "b56c695f-ea02-4aa0-b400-4550263e8c23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4176\n",
            "['피크닉가방', '휴진', '촬영', '드론역할', '택배지연', '갤럭시', '워터파크', '상큼함', '원', '퍼스널컬러테스트', '아동', '해수욕', '아르타조나', '신선한', '서식하다', '떡메모지', '인기곡', '상승', '계산서', '인재']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#xml to json"
      ],
      "metadata": {
        "id": "YY6uQxryCF0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same with https://github.com/miridi-sanhak/model/blob/main/preprocessing.py\n",
        "\n",
        "import json\n",
        "import math\n",
        "from io import BytesIO\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xml_to_dict\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def process_bbox(XML_BBOX, IM_SIZE, SHEET_SIZE, angle, center):\n",
        "    RATIO = IM_SIZE[0] / SHEET_SIZE[0]\n",
        "    x1, y1, x2, y2 = map(float, XML_BBOX)\n",
        "    x1, y1, x2, y2 = (x1 * RATIO, y1 * RATIO, x2 * RATIO, y2 * RATIO)\n",
        "    center = (center[0] * RATIO, center[1] * RATIO)\n",
        "\n",
        "    if angle != 0:\n",
        "        angle = 360 - angle\n",
        "        angle = math.radians(angle)\n",
        "        # Calculate the center point of the bbox\n",
        "        center_x, center_y = center\n",
        "        # Calculate the distance from the center to each corner of the bbox\n",
        "        distance_x = (x1 - center_x)\n",
        "        distance_y = (y1 - center_y)\n",
        "        # Apply rotation to the distances\n",
        "        new_distance_x = distance_x * math.cos(angle) - distance_y * math.sin(angle)\n",
        "        new_distance_y = distance_x * math.sin(angle) + distance_y * math.cos(angle)\n",
        "        # Calculate the new corners after rotation\n",
        "        x1 = center_x + new_distance_x\n",
        "        y1 = center_y + new_distance_y\n",
        "        x2 = center_x - new_distance_x\n",
        "        y2 = center_y - new_distance_y\n",
        "\n",
        "    x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
        "\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def get_render_bbox(text):\n",
        "    if text['RenderPos'] == None:\n",
        "        return []\n",
        "    render_pos = json.loads(text['RenderPos'])\n",
        "    render_bbox = []\n",
        "\n",
        "    left, top, right, bottom = map(float, text['Position'].values())\n",
        "\n",
        "    for render in render_pos['c']:\n",
        "        x, a, w, y = map(float, [render['x'], render['a'], render['w'], render['y']])\n",
        "        left_ = left + x\n",
        "        right_ = left_ + w\n",
        "        bottom_ = top + y\n",
        "        top_ = bottom_ - a\n",
        "\n",
        "        render_bbox.append((left_, top_, right_, bottom_))\n",
        "\n",
        "    return render_bbox\n",
        "\n",
        "def get_bbox(render_bbox):\n",
        "    min_x = min([x[0] for x in render_bbox])\n",
        "    min_y = min([x[1] for x in render_bbox])\n",
        "    max_x = max([x[2] for x in render_bbox])\n",
        "    max_y = max([x[3] for x in render_bbox])\n",
        "\n",
        "    return min_x, min_y, max_x, max_y\n",
        "\n",
        "def process_xml_dict(xml_dict, thumbnail):\n",
        "    processed_json = {}\n",
        "    processed_json['form'] = []\n",
        "\n",
        "    SHEET_SIZE = tuple(map(int, xml_dict['SHEET']['SHEETSIZE'].values()))\n",
        "    IM_SIZE = thumbnail.size\n",
        "\n",
        "    # Process XML to json\n",
        "    for i, text in enumerate(xml_dict['SHEET']['TEXT']):\n",
        "        left, top, right, bottom = map(float, text['Position'].values())\n",
        "        center = ((left + right) / 2, (top + bottom) / 2)\n",
        "\n",
        "        render_bbox = get_render_bbox(text)\n",
        "        if len(render_bbox) == 0: continue\n",
        "\n",
        "        XML_BBOX = get_bbox(render_bbox)\n",
        "\n",
        "        t = text['Text']\n",
        "        x1, y1, x2, y2 = process_bbox(XML_BBOX, IM_SIZE, SHEET_SIZE, int(text['@Rotate']), center)\n",
        "\n",
        "        processed_json['form'].append({\n",
        "            \"text\": t,\n",
        "            \"box\": [x1, y1, x2, y2],\n",
        "            \"font_id\": int(text['Font']['@FamilyIdx']),\n",
        "            \"font_size\": float(text['Font']['@Size']),\n",
        "            \"style\": {\n",
        "                \"bold\": text['Font']['Style']['@Bold'] == 'true',\n",
        "                \"italic\": text['Font']['Style']['@Italic'] == 'true',\n",
        "                \"strikeout\": text['Font']['Style']['@Strikeout'] == 'true',\n",
        "                \"underline\": text['Font']['Style']['@Underline'] == 'true'\n",
        "            },\n",
        "            \"linespace\": float(text['Font']['@LineSpace']),\n",
        "            \"opacity\": float(text['@Opacity']),\n",
        "            \"rotate\": float(text['@Rotate']),\n",
        "            \"id\": i\n",
        "        })\n",
        "\n",
        "        processed_json['form'][-1]['words'] = []\n",
        "\n",
        "        render_pos = json.loads(text['RenderPos'])\n",
        "\n",
        "        for j, bbox in enumerate(render_bbox):\n",
        "            x1_, y1_, x2_, y2_ = process_bbox(bbox, IM_SIZE, SHEET_SIZE, int(text['@Rotate']), center)\n",
        "            color = render_pos['c'][j]['f']\n",
        "            color = color[4:-1]\n",
        "            color = list(map(int, color.split(\",\")))\n",
        "            processed_json['form'][-1]['words'].append({\n",
        "                \"text\": render_pos['c'][j]['t'],\n",
        "                \"box\": [x1_, y1_, x2_, y2_],\n",
        "                \"font_size\": float(render_pos['c'][j]['s']),\n",
        "                \"letter_spacing\": float(render_pos['c'][j]['ds']),\n",
        "                \"font_id\": int(render_pos['c'][j]['yd']),\n",
        "                \"color\": color\n",
        "            })\n",
        "\n",
        "    return processed_json\n",
        "\n",
        "def process_xml(sheet_url, thumbnail_url):\n",
        "    sample_thumbnail = Image.open(BytesIO(requests.get(thumbnail_url).content))\n",
        "    sample_xml = requests.get(sheet_url).content.decode(\"utf-8\")\n",
        "    sample_json = xml_to_dict.XMLtoDict().parse(sample_xml)\n",
        "\n",
        "    processed_json = process_xml_dict(sample_json, sample_thumbnail)\n",
        "\n",
        "    return processed_json\n",
        "\n",
        "def make_sample_json():\n",
        "    # Read sample CSV and download thumbnail, XML\n",
        "    df = pd.read_csv(xml_sample_loc)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "\n",
        "      sample_sheet = df.iloc[i]\n",
        "\n",
        "      try:\n",
        "        processed_json = process_xml(sample_sheet['sheet_url'], sample_sheet['thumbnail_url'])\n",
        "\n",
        "        filename = f\"data/processed_sample_{i}.json\"\n",
        "        with open(filename, \"w\") as file_:\n",
        "          json.dump(processed_json, file_, indent=4)\n",
        "      except:\n",
        "        print(f\"error occurred in df line {i}\")"
      ],
      "metadata": {
        "id": "7jUZ5m6BCHFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_sample_json() #약 30분 소요. xml파일을 json으로 변환"
      ],
      "metadata": {
        "id": "IK0ecmL5CTG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get text line in xml file"
      ],
      "metadata": {
        "id": "3poxvfRhCXI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#json파일에서 text들을 가져와 text_sum에 저장\n",
        "import json\n",
        "import os\n",
        "\n",
        "text_sum = []\n",
        "\n",
        "folder_path = \"data/\"  # 데이터 폴더 경로\n",
        "\n",
        "# 폴더 내의 파일 목록 가져오기\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith(\".json\"):  # .json 파일인 경우에만 처리\n",
        "        file_path = os.path.join(folder_path, file_name)  # 파일 경로 생성\n",
        "\n",
        "    try:\n",
        "      with open(file_path, \"r\") as file_:\n",
        "          data = json.load(file_)\n",
        "\n",
        "      for i in range(len(data['form'])):\n",
        "          if(data['form'][i]['text']!=None):\n",
        "            text_sum.append(data['form'][i]['text'])  # 요소 추가\n",
        "\n",
        "    except:\n",
        "      print(f\"error in {file_path}\")\n",
        "\n",
        "\n",
        "print(len(text_sum))\n",
        "print(text_sum[0])"
      ],
      "metadata": {
        "id": "_fw2PM9qCZNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train custom tokenizer (based on ke-t5)"
      ],
      "metadata": {
        "id": "66ZWFovSCkOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenization_data = np.concatenate((text_sum,keyword_unique)) #text와 keyword 합쳐서 한번에 train\n",
        "\n",
        "print(len(train_tokenization_data))\n",
        "\n",
        "def get_training_corpus(): #개수가 많으므로 1000개씩 끊어서 return\n",
        "    return (\n",
        "        train_tokenization_data[i : i + 1000]\n",
        "        for i in range(0, len(train_tokenization_data), 1000)\n",
        "    )\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "QuK8UyKICcRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_tokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR/ke-t5-base-ko\") #pre-trained된거 불러오기\n",
        "old_tokenizer.vocab_size #64100"
      ],
      "metadata": {
        "id": "-2tiMdZ3CpMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 15000) #뒤 숫자는 우리 데이터에서 단어 개수\n",
        "tokenizer.vocab_size #14602\n",
        "tokenizer.save_pretrained(\"tokenizer_finetuned_ket5_by_xml_data\") #저장"
      ],
      "metadata": {
        "id": "Fc584Ff3CupF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#see tokenized result (추후 학습과 관련 x)"
      ],
      "metadata": {
        "id": "f5L7gXTnDM9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized by finetuned-ke-t5\n",
        "#text_sum 안의 text들을 토큰화하여 text_token_list에 저장.\n",
        "\n",
        "text_token_list = np.array([],dtype=object)\n",
        "\n",
        "for idx , word in enumerate(train_tokenization_data):\n",
        "  input_word = word\n",
        "  try:\n",
        "    tokens = tokenizer.tokenize(input_word)\n",
        "    text_token_list = np.concatenate((text_token_list,tokens))\n",
        "  except:\n",
        "    print(f\"error occur in input_word {input_word}, idx = {idx}\")\n",
        "\n",
        "print(text_token_list[:100])"
      ],
      "metadata": {
        "id": "gmrA993oCycz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keyword token과 text token들을 합쳐서 중복제거.\n",
        "\n",
        "token_list = np.concatenate((text_token_list,keyword_unique))\n",
        "\n",
        "token_list = list(set(token_list))\n",
        "\n",
        "print(len(token_list) ) #==14602\n",
        "\n",
        "print(token_list[:20])"
      ],
      "metadata": {
        "id": "8zj2zkgtC1SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TLTUiNUTDSsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#define custom tokenizer class"
      ],
      "metadata": {
        "id": "LmJo1DsmEAev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5TokenizerFast, PreTrainedTokenizer, PreTrainedTokenizerBase\n",
        "\n",
        "import re\n",
        "import sentencepiece as spm\n",
        "\n",
        "# The special tokens of T5Tokenizer is hard-coded with <extra_id_{}>\n",
        "# Created another class UDOPTokenizer extending it to add special visual tokens like <loc_{}>, etc.\n",
        "\n",
        "#class UdopTokenizer(T5Tokenizer):\n",
        "class UdopTokenizer(AutoTokenizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        eos_token=\"</s>\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        extra_ids=100,\n",
        "        loc_extra_ids=501,\n",
        "        other_extra_ids=200,\n",
        "        additional_special_tokens=[],\n",
        "        sp_model_kwargs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # Add extra_ids to the special token list\n",
        "        if extra_ids > 0 and not \"<extra_id_0>\" in additional_special_tokens:\n",
        "            additional_special_tokens = [\"<extra_id_{}>\".format(i) for i in range(extra_ids)]\n",
        "            additional_special_tokens.extend([\"<extra_l_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "            additional_special_tokens.extend([\"</extra_l_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "            additional_special_tokens.extend([\"<extra_t_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "            additional_special_tokens.extend([\"</extra_t_id_{}>\".format(i) for i in range(extra_ids)])\n",
        "\n",
        "        if loc_extra_ids > 0 and not \"<loc_0>\" in additional_special_tokens:\n",
        "            additional_special_tokens.extend([\"<loc_{}>\".format(i) for i in range(loc_extra_ids)])\n",
        "\n",
        "        if other_extra_ids > 0 and not \"<other_0>\" in additional_special_tokens:\n",
        "            additional_special_tokens.extend([\"<other_{}>\".format(i) for i in range(other_extra_ids)])\n",
        "        print(additional_special_tokens)\n",
        "        PreTrainedTokenizer.__init__(\n",
        "            self,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            pad_token=pad_token,\n",
        "            extra_ids=extra_ids,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n",
        "\n",
        "        self.vocab_file = vocab_file\n",
        "        self._extra_ids = extra_ids\n",
        "        self._loc_extra_ids = loc_extra_ids\n",
        "        self._other_extra_ids = other_extra_ids\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self.sp_model.get_piece_size() + self._extra_ids * 5 + self._loc_extra_ids + self._other_extra_ids\n",
        "\n",
        "    def get_vocab(self):\n",
        "        vocab = {self.convert_ids_to_tokens(\n",
        "            i): i for i in range(self.vocab_size)}\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "        if token.startswith(\"<extra_id_\"):\n",
        "            match = re.match(r\"<extra_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 4\n",
        "        elif token.startswith(\"<extra_l_id_\"):\n",
        "            match = re.match(r\"<extra_l_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 3\n",
        "        elif token.startswith(\"</extra_l_id_\"):\n",
        "            match = re.match(r\"</extra_l_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 2\n",
        "        elif token.startswith(\"<extra_t_id_\"):\n",
        "            match = re.match(r\"<extra_t_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids\n",
        "        elif token.startswith(\"</extra_t_id_\"):\n",
        "            match = re.match(r\"</extra_t_id_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids\n",
        "        elif token.startswith(\"<loc_\"):\n",
        "            match = re.match(r\"<loc_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1 - self._other_extra_ids\n",
        "        elif token.startswith(\"<other_\"):\n",
        "            match = re.match(r\"<other_(\\d+)>\", token)\n",
        "            num = int(match.group(1))\n",
        "            return self.vocab_size - num - 1\n",
        "\n",
        "        return self.sp_model.piece_to_id(token)\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        if index < self.sp_model.get_piece_size():\n",
        "            token = self.sp_model.IdToPiece(index)\n",
        "        else:\n",
        "\n",
        "            if index > self.sp_model.get_piece_size() + self._extra_ids * 5 + self._loc_extra_ids - 1:\n",
        "                index_loc = self.vocab_size - 1 - index\n",
        "                token = f\"<other_{index_loc}>\"\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 5 - 1:\n",
        "                index_loc = self.vocab_size - self._other_extra_ids - 1 - index\n",
        "                token = f\"<loc_{index_loc}>\"\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 4 - 1:\n",
        "                token = \"</extra_t_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 3 - 1:\n",
        "                token = \"<extra_t_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids * 2 - 1:\n",
        "                token = \"</extra_l_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 2 - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() + self._extra_ids - 1:\n",
        "                token = \"<extra_l_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 3 - 1 - index)\n",
        "            elif index > self.sp_model.get_piece_size() - 1:\n",
        "                token = \"<extra_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 4 - 1 - index)\n",
        "            else:\n",
        "                raise\n",
        "        return token"
      ],
      "metadata": {
        "id": "R3v22ARiECUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test tokenizer class"
      ],
      "metadata": {
        "id": "r2IgTNFREGiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = UdopTokenizer.from_pretrained(\"tokenizer_finetuned_ket5_by_xml_data\")"
      ],
      "metadata": {
        "id": "W9kAEC08EFBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer.vocab_size\n",
        "my_tokenizer.decode([4321,1231,1050])"
      ],
      "metadata": {
        "id": "mqDECyjoEJlK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}