{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1GRDVaZoDkXoDAX6s5F3maCOMOL-g4khy","authorship_tag":"ABX9TyP6JvUg+GqiMDM3H0iRDjnA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["masking을 어떤 것을 하느냐가 성능에 영향을 줄 것.\n","\n","개별 token을 그냥 masking하는 것이 오히려 좋은 성능을 보일 수도.\n","\n","아니면 keyword 추출하는데 영향을 미친 token을 masking하는 방식?\n","\n","bounding box 단위? token 단위? masing 어떻게 할까\n","\n","tokenizer 다르게 씀으로 인해 주요 단어들이 UNK로 튕겨나갈 수 있다.\n","\n","--> tokenizer를 바꾸고 새로 학습?\n","\n","--> tokenizer t5 그냥 쓰고 뒤에 한국어 tokenizer붙여 vocab 크기를 늘리는 방식?\n","\n","tokenizer의 학습: vocab의 빈도를 학습. 빈도가 크면 그냥 사용. 빈도가 작으면 vocab을 잘라서 사용."],"metadata":{"id":"x6gcGoDbYrES"}},{"cell_type":"code","source":["\n","!pip install sentencepiece\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFyTXs2sRhd0","executionInfo":{"status":"ok","timestamp":1687759000278,"user_tz":-540,"elapsed":15217,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}},"outputId":"3dc4e09b-7ef1-43eb-a2b0-7b9f244ec93f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","import random\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Union"],"metadata":{"id":"a9lSOBxtRlA_","executionInfo":{"status":"ok","timestamp":1687761023337,"user_tz":-540,"elapsed":1259,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5TokenizerFast, PreTrainedTokenizer, PreTrainedTokenizerBase\n","\n","import re\n","import sentencepiece as spm\n","\n","# The special tokens of T5Tokenizer is hard-coded with <extra_id_{}>\n","# Created another class UDOPTokenizer extending it to add special visual tokens like <loc_{}>, etc.\n","\n","#class UdopTokenizer(T5Tokenizer):\n","class UdopTokenizer(AutoTokenizer):\n","\n","    def __init__(\n","        self,\n","        vocab_file,\n","        eos_token=\"</s>\",\n","        unk_token=\"<unk>\",\n","        pad_token=\"<pad>\",\n","        extra_ids=100,\n","        loc_extra_ids=501,\n","        other_extra_ids=200,\n","        additional_special_tokens=[],\n","        sp_model_kwargs=None,\n","        **kwargs\n","    ):\n","        # Add extra_ids to the special token list\n","        if extra_ids > 0 and not \"<extra_id_0>\" in additional_special_tokens:\n","            additional_special_tokens = [\"<extra_id_{}>\".format(i) for i in range(extra_ids)]\n","            additional_special_tokens.extend([\"<extra_l_id_{}>\".format(i) for i in range(extra_ids)])\n","            additional_special_tokens.extend([\"</extra_l_id_{}>\".format(i) for i in range(extra_ids)])\n","            additional_special_tokens.extend([\"<extra_t_id_{}>\".format(i) for i in range(extra_ids)])\n","            additional_special_tokens.extend([\"</extra_t_id_{}>\".format(i) for i in range(extra_ids)])\n","\n","        if loc_extra_ids > 0 and not \"<loc_0>\" in additional_special_tokens:\n","            additional_special_tokens.extend([\"<loc_{}>\".format(i) for i in range(loc_extra_ids)])\n","\n","        if other_extra_ids > 0 and not \"<other_0>\" in additional_special_tokens:\n","            additional_special_tokens.extend([\"<other_{}>\".format(i) for i in range(other_extra_ids)])\n","        print(additional_special_tokens)\n","        PreTrainedTokenizer.__init__(\n","            self,\n","            eos_token=eos_token,\n","            unk_token=unk_token,\n","            pad_token=pad_token,\n","            extra_ids=extra_ids,\n","            additional_special_tokens=additional_special_tokens,\n","            **kwargs,\n","        )\n","\n","        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n","\n","        self.vocab_file = vocab_file\n","        self._extra_ids = extra_ids\n","        self._loc_extra_ids = loc_extra_ids\n","        self._other_extra_ids = other_extra_ids\n","\n","        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return self.sp_model.get_piece_size() + self._extra_ids * 5 + self._loc_extra_ids + self._other_extra_ids\n","\n","    def get_vocab(self):\n","        vocab = {self.convert_ids_to_tokens(\n","            i): i for i in range(self.vocab_size)}\n","        vocab.update(self.added_tokens_encoder)\n","        return vocab\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n","        if token.startswith(\"<extra_id_\"):\n","            match = re.match(r\"<extra_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 4\n","        elif token.startswith(\"<extra_l_id_\"):\n","            match = re.match(r\"<extra_l_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 3\n","        elif token.startswith(\"</extra_l_id_\"):\n","            match = re.match(r\"</extra_l_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 2\n","        elif token.startswith(\"<extra_t_id_\"):\n","            match = re.match(r\"<extra_t_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids - self._extra_ids\n","        elif token.startswith(\"</extra_t_id_\"):\n","            match = re.match(r\"</extra_t_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._other_extra_ids - self._loc_extra_ids\n","        elif token.startswith(\"<loc_\"):\n","            match = re.match(r\"<loc_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._other_extra_ids\n","        elif token.startswith(\"<other_\"):\n","            match = re.match(r\"<other_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1\n","\n","        return self.sp_model.piece_to_id(token)\n","\n","    def _convert_id_to_token(self, index):\n","        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n","        if index < self.sp_model.get_piece_size():\n","            token = self.sp_model.IdToPiece(index)\n","        else:\n","\n","            if index > self.sp_model.get_piece_size() + self._extra_ids * 5 + self._loc_extra_ids - 1:\n","                index_loc = self.vocab_size - 1 - index\n","                token = f\"<other_{index_loc}>\"\n","            elif index > self.sp_model.get_piece_size() + self._extra_ids * 5 - 1:\n","                index_loc = self.vocab_size - self._other_extra_ids - 1 - index\n","                token = f\"<loc_{index_loc}>\"\n","            elif index > self.sp_model.get_piece_size() + self._extra_ids * 4 - 1:\n","                token = \"</extra_t_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - 1 - index)\n","            elif index > self.sp_model.get_piece_size() + self._extra_ids * 3 - 1:\n","                token = \"<extra_t_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids - 1 - index)\n","            elif index > self.sp_model.get_piece_size() + self._extra_ids * 2 - 1:\n","                token = \"</extra_l_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 2 - 1 - index)\n","            elif index > self.sp_model.get_piece_size() + self._extra_ids - 1:\n","                token = \"<extra_l_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 3 - 1 - index)\n","            elif index > self.sp_model.get_piece_size() - 1:\n","                token = \"<extra_id_{}>\".format(self.vocab_size - self._other_extra_ids - self._loc_extra_ids - self._extra_ids * 4 - 1 - index)\n","            else:\n","                raise\n","        return token\n","\n"],"metadata":{"id":"iefjPNJKJJLQ","executionInfo":{"status":"ok","timestamp":1687761023767,"user_tz":-540,"elapsed":2,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"\n","    codes for just preprocessing\n","\"\"\"\n","\n","def normalText(t):\n","    if type(t) is float:\n","        if t == int(t):\n","            t = int(t)\n","    t = str(t)\n","    return t.strip()\n","\n","\n","def get_prop(node, name):\n","    title = node.get(\"title\")\n","    props = title.split(\";\")\n","    for prop in props:\n","        (key, args) = prop.split(None, 1)\n","        args = args.strip('\"')\n","        if key == name:\n","            return args\n","    return None\n","\n","\n","def get_bb(bb):\n","    bbs = [float(j) for j in bb]\n","    xs, ys = [], []\n","    for i, b in enumerate(bbs):\n","        if i % 2 == 0:\n","            xs.append(b)\n","        else:\n","            ys.append(b)\n","    return [min(xs), min(ys), max(xs), max(ys)]\n","\n","\n","from PIL import Image\n","import torchvision.transforms as T\n","from torchvision.transforms import functional as F\n","\n","\"\"\"\n","    codes for just preprocessing image data\n","\"\"\"\n","def get_visual_bbox(image_size=224):\n","    image_feature_pool_shape = [image_size//16, image_size//16]\n","    visual_bbox_x = (torch.arange(\n","        0,\n","        1.0 * (image_feature_pool_shape[1] + 1),\n","        1.0,\n","    ) / image_feature_pool_shape[1])\n","    visual_bbox_y = (torch.arange(\n","        0,\n","        1.0 * (image_feature_pool_shape[0] + 1),\n","        1.0,\n","    ) / image_feature_pool_shape[0])\n","    visual_bbox_input = torch.stack(\n","        [\n","            visual_bbox_x[:-1].repeat(\n","                image_feature_pool_shape[0], 1),\n","            visual_bbox_y[:-1].repeat(\n","                image_feature_pool_shape[1], 1).transpose(\n","                    0, 1),\n","            visual_bbox_x[1:].repeat(\n","                image_feature_pool_shape[0], 1),\n","            visual_bbox_y[1:].repeat(\n","                image_feature_pool_shape[1], 1).transpose(\n","                    0, 1),\n","        ],\n","        dim=-1,\n","    ).view(-1, 4)\n","    return visual_bbox_input\n","\n","class Normalize(object):\n","    def __init__(self, mean, std, format='rgb'):\n","        self.mean = mean\n","        self.std = std\n","        self.format = format.lower()\n","\n","    def __call__(self, image):\n","        if 'bgr' in self.format:\n","            image = image[[2, 1, 0]]\n","        if '255' in self.format:\n","            image = image * 255\n","        if image.size(0) == 1:\n","            image = image.repeat(3, 1, 1)\n","        image = F.normalize(image, mean=self.mean, std=self.std)\n","        return image\n","\n","def img_trans_torchvision(image, image_size=224):\n","    trans = T.Compose([\n","            T.Resize([image_size,image_size]),\n","            T.ToTensor(),\n","            Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )])\n","\n","    image = trans(image)  # copy to make it writeable\n","    return image"],"metadata":{"id":"QYK3W-IWJNyN","executionInfo":{"status":"ok","timestamp":1687761026331,"user_tz":-540,"elapsed":544,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VLmNHtucISfi","executionInfo":{"status":"ok","timestamp":1687761027432,"user_tz":-540,"elapsed":3,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}}},"outputs":[],"source":["class DataCollatorForT5DocCLS:\n","    \"\"\"\n","    Data collator used for T5 document classification\n","    \"\"\"\n","    def __init__(self, json_data=None, tokenizer=None, meta_path=None, input_length=None, target_length=None, pad_token_id=None, decoder_start_token_id=None):\n","\n","        self.json_data = json_data\n","        self.tokenizer = tokenizer #이전에 만든 udop tokenizer를 불러옴\n","        self.input_length = input_length\n","        self.target_length = target_length\n","        self.pad_token_id = pad_token_id\n","        self.decoder_start_token_id = decoder_start_token_id\n","\n","    def __call__(self, user_prompt ,ori_input_ids, ori_bbox_list, labels=None):\n","\n","        # \"원래 input text 정보 & bounding box\"\n","        # -->\n","        # \"prompt text 정보 + 원래 input text 정보\" list\n","        # +\n","        # [0,0,0,0]을 promt text token 개수만큼 + 원래 bounding box\n","\n","        #prompt_text = 'document classification.'\n","        prompt_text = user_prompt\n","        prompt_ids =  self.tokenizer.encode(prompt_text, add_special_tokens=False)\n","        input_ids = prompt_ids + ori_input_ids\n","        bbox_list = [[0,0,0,0]] * len(prompt_ids) + ori_bbox_list\n","\n","        if(labels!=None):  #label은 classification에서만 수행\n","        #인줄 알았는데 layout modeling 이런것도 다 output이 있으니까 label==output 인건가..???\n","          labels = self.tokenizer.encode(labels, add_special_tokens=True)\n","\n","        return input_ids, labels, bbox_list\n","\n","\n","# 해당 부분은 json파일의 문장을 line-by-line으로 읽는 것으로 해당 함수 수정 완료.\n","def read_ocr_core_engine(json_data,image_path, file_idx, tokenizer, max_seq_length=None, num_img_embeds=None, image_size=224):\n","    #max_seq_length와 num_img_embeds 는 원본 코드에서도 안쓰는데 왜있는거지?\n","\n","    #file_ 와 image_dir 모두 1 2 3 ... index 임.\n","\n","    data = json_data[file_idx] # 하나로 합쳐진 json파일에서 현재 idx 가져옴.\n","\n","    rets = []\n","    n_split = 0\n","\n","    #page_size = (1280,720)\n","    page_size = data['form'][0]['sheet_size']\n","\n","    tiff_images =  Image.open(f\"{image_path}/image_{file_idx}.png\")\n","\n","    image = img_trans_torchvision(tiff_images, image_size)\n","\n","    text_list, bbox_list = [], []\n","    for form in data['form']: #문장별로 쪼갬\n","      for word in form['words']: #단어별로 쪼갬\n","\n","        if word == ' ': #띄어쓰기는 건너뛰기\n","          continue\n","\n","        sub_tokens = tokenizer.tokenize(word['text']) #단어별로 쪼갠걸 다시 토큰화 (하나의 단어도 여러개의 토큰 가능)\n","        for sub_token in sub_tokens:\n","          text_list.append(sub_token)\n","          bbox_list.append(word['box']) #현재는 단어별 bbox, 추후 문장별 bbox로도 수정 가능\n","          #bbox_list.append(form['box'])\n","\n","    if len(text_list) > 0:\n","      rets.append([text_list, bbox_list, image, page_size])\n","\n","    assert len(text_list) == len(bbox_list)\n","    n_split = len(rets)\n","\n","    return rets, n_split\n","\n","\n","import pandas as pd\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","\n","\n","EMPTY_BOX = [0, 0, 0, 0]\n","SEP_BOX = [1000, 1000, 1000, 1000]\n","\n","class RvlCdipDataset(Dataset):\n","\n","    #NUM_LABELS = 16\n","\n","    def __init__(self , xml_sample_loc , json_sum_loc, image_path, tokenizer , data_args , mode='train'):\n","\n","        \"\"\" Structure of data directory:\n","\n","            --- xml_sample_loc (.csv)\n","                   ├── images_url\n","                   └── labels_url\n","            --- data (folder)\n","                   └── processed_sample{index} .json\n","        \"\"\"\n","        self.main_df = pd.read_csv(xml_sample_loc) # xml_sample.csv 파일 저장\n","        self.image_path = image_path\n","\n","        with open(json_sum_loc, 'r', encoding='utf8') as f:\n","            self.main_json_data = json.load(f)\n","\n","        if mode == 'train': #train ,val, test 에 따라 사용하는 data의 범위가 다름. (근데 self-supervised도 이거 필요 있나..? )\n","            file_data_range = ( 0 , int(len(self.main_df) * 0.6 ) )\n","        elif mode == 'val':\n","            file_data_range = ( int(len(self.main_df) * 0.6 ) , int(len(self.main_df) * 0.8 ) )\n","        elif mode == 'test':\n","            file_data_range = ( int(len(self.main_df) * 0.8 ) , int(len(self.main_df) ) )\n","        else:\n","            raise NotImplementedError\n","\n","        self.cls_bbox = EMPTY_BOX[:]\n","        self.pad_bbox = EMPTY_BOX[:]\n","        self.sep_bbox = SEP_BOX[:]\n","\n","        self.tokenizer = tokenizer\n","        self.max_seq_length = data_args.max_seq_length\n","        self.num_img_embeds = 0\n","\n","        label_list = ['a','b'] #get_rvlcdip_labels() #classification task에서만 사용하는 변수\n","        self.label_list = label_list\n","        self.label_map = dict(zip(list(range(len(self.label_list))), self.label_list))\n","        self.n_classes = len(label_list)\n","        self.label_list = label_list\n","\n","        self.image_size = data_args.image_size\n","\n","        self.examples = []\n","        self.labels = []\n","        self.images = []\n","\n","        self.cls_collator = DataCollatorForT5DocCLS( #기존에 정의한 토크나이저 선언\n","                json_data = self.main_json_data ,\n","                  tokenizer=tokenizer,\n","            )\n","\n","        results = [self.load_file(file_idx) for file_idx in tqdm(range(file_data_range[0],file_data_range[1]))]\n","        for labels, examples, images in results:\n","            self.labels += labels\n","            self.examples += examples\n","            self.images += images\n","        assert len(self.labels) == len(self.examples)\n","\n","    def load_file(self, file_idx):\n","\n","        labels = []\n","        examples = []\n","        images = []\n","\n","        labels.append(0) ############### label 미정으로 일단 다 0 ##########\n","        examples.append(file_idx)\n","        images.append(file_idx)\n","\n","        return labels, examples, images\n","\n","    def __getitem__(self, index): #완료\n","        try:\n","            label = self.labels[index]\n","            label = self.label_map[int(label)]\n","\n","            rets, n_split = read_ocr_core_engine(json_data = self.main_json_data , image_path = self.image_path , file_idx = index , tokenizer = self.tokenizer, max_seq_length = self.max_seq_length, num_img_embeds = self.num_img_embeds, image_size = self.image_size)\n","\n","            if n_split == 0:\n","                # Something wrong with the .ocr.json file\n","                print(f\"EMPTY ENTRY in index {index}\")\n","                return self[(index + 1) % len(self)]\n","            for i in range(n_split): #정상적으로 코드 실행됬다면 n_split==1 임.\n","                text_list, bbox_list, image, page_size = rets[i]\n","                (width, height) = page_size\n","                bbox = [  #이미지 크기에 맞게 정규화\n","                    [\n","                        b[0] / width,\n","                        b[1] / height,\n","                        b[2] / width,\n","                        b[3] / height,\n","                    ]\n","                    for b in bbox_list\n","                ]\n","\n","                visual_bbox_input = get_visual_bbox(self.image_size) # (x_min, y_min, x_max, y_max) 형태의 좌표로 이루어진 텐서 반환\n","\n","                input_ids = self.tokenizer.convert_tokens_to_ids(text_list) #토큰 자른것들을 token id들로 변환\n","\n","                input_ids, labels, bbox_input = self.cls_collator(\"user prompt\", input_ids, bbox, label) #prompt 붙여서 최종 input,bbox,label을 만듦. ################################\n","                attention_mask = [1] * len(input_ids)\n","                decoder_attention_mask = [1] * len(labels)\n","\n","                char_list = [0]\n","                char_bbox_list = [[0,0,0,0]]\n","                char_ids = torch.tensor(char_list, dtype=torch.long)\n","                char_bbox_input = torch.tensor(char_bbox_list, dtype=torch.float)\n","\n","                bbox_input = torch.tensor(bbox_input, dtype=torch.float)\n","                labels = torch.tensor(labels, dtype=torch.long)\n","                input_ids = torch.tensor(input_ids, dtype=torch.long)\n","                attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n","                decoder_attention_mask = torch.tensor(decoder_attention_mask, dtype=torch.long)\n","                assert len(bbox_input) == len(input_ids)\n","                assert len(bbox_input.size()) == 2\n","                assert len(char_bbox_input.size()) == 2\n","\n","                return_dict =  {\n","                    \"input_ids\": input_ids,\n","                    \"attention_mask\": attention_mask,\n","                    \"labels\": labels,\n","                    \"seg_data\": bbox_input,\n","                    \"visual_seg_data\": visual_bbox_input,\n","                    \"decoder_attention_mask\": decoder_attention_mask,\n","                    \"image\": image,\n","                    'char_ids': char_ids,\n","                    'char_seg_data': char_bbox_input\n","                }\n","                assert input_ids is not None\n","\n","                return return_dict\n","        except: #오류가 났다는 거는 파일이 없다는 것. 해당 상황에서는 index+1 파일 불러오는 것으로 대체\n","            #image는 로딩 중 오류 생긴 파일 그냥 해당 index가 없게 저장해서 문제 없음.\n","            #json파일도 오류 생긴건 해당 index없어서 걸러짐.\n","            print(f\"{index} 파일을 {0}로 대체\")\n","\n","            return self.__getitem__(0)\n","\n","            #return self[(index + 1) % len(self)]\n","\n","    #def get_labels(self): # classification에서 label의 종류 출력하는 함수. 우리는 필요 없을 듯.\n","    #    return list(map(str, list(range(self.NUM_LABELS))))\n","\n","    def pad_tokens(self, input_ids, bbox): #이건 그냥 길이 max_len에 맞게 맞추는 함수\n","        # [CLS], sentence, [SEP]\n","        tokenized_tokens = self.tokenizer.build_inputs_with_special_tokens(input_ids)\n","        start_token, _, end_token = tokenized_tokens[0], tokenized_tokens[1:-1], tokenized_tokens[-1]\n","\n","        sentence = tokenized_tokens\n","        expected_seq_length = self.max_seq_length - self.num_img_embeds\n","        mask = torch.zeros(expected_seq_length)\n","        mask[:len(sentence)] = 1\n","\n","        bbox = [self.cls_bbox] + bbox + [self.sep_bbox]\n","        while len(sentence) < expected_seq_length:\n","            sentence.append(self.tokenizer.pad_token_id)\n","            bbox.append(self.pad_bbox)\n","\n","        assert len(sentence) == len(bbox)\n","        return (sentence, mask, bbox, start_token, end_token)"]},{"cell_type":"markdown","source":["#test"],"metadata":{"id":"OC72ZCezc6AU"}},{"cell_type":"code","source":["import json\n","\n","xml_sample_loc = '/content/drive/MyDrive/ColabNotebooks/산학_테스트/xml_sample_20230519.csv'\n","json_sum_loc = '/content/drive/MyDrive/ColabNotebooks/산학_테스트/sumall_processed_sample.json'\n","image_path = '/content/drive/MyDrive/ColabNotebooks/산학_테스트/image' #folder 경로임."],"metadata":{"id":"yKGoPXf1fvQp","executionInfo":{"status":"ok","timestamp":1687761030400,"user_tz":-540,"elapsed":281,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import argparse\n","\n","# data_args를 Namespace 객체로 변환합니다.\n","data_args = argparse.Namespace(\n","    max_seq_length=500,\n","    image_size=224\n",")\n","#json 하나로 묶어서 pikkle로 대체\n","my_tokenizer = UdopTokenizer.from_pretrained(\"/content/drive/MyDrive/ColabNotebooks/산학_테스트/tokenizer_finetuned_ket5_by_xml_data\")\n","\n","dataset = RvlCdipDataset(xml_sample_loc , json_sum_loc , image_path,  my_tokenizer , data_args , mode='train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llxXWBHcVlgT","executionInfo":{"status":"ok","timestamp":1687761036152,"user_tz":-540,"elapsed":5351,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}},"outputId":"b5a25794-4e50-427d-d694-6b3a0b6e1184"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1844/1844 [00:00<00:00, 342801.90it/s]\n"]}]},{"cell_type":"code","source":["for i in range(1844): #1800개 9분 걸림. drvie에서 불러오는 상황. local에서 불러오면 더 빠를 것.\n","  x = dataset.__getitem__(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRa_ILhtWC-A","executionInfo":{"status":"ok","timestamp":1687761621285,"user_tz":-540,"elapsed":549815,"user":{"displayName":"소프트웨어학과/김종효","userId":"02917922017757632153"}},"outputId":"21164b59-64fb-41e7-f22f-849f8eeb030e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["4 파일을 0로 대체\n","8 파일을 0로 대체\n","13 파일을 0로 대체\n","44 파일을 0로 대체\n","80 파일을 0로 대체\n","82 파일을 0로 대체\n","84 파일을 0로 대체\n","86 파일을 0로 대체\n","382 파일을 0로 대체\n","486 파일을 0로 대체\n","767 파일을 0로 대체\n","932 파일을 0로 대체\n","979 파일을 0로 대체\n","980 파일을 0로 대체\n","1013 파일을 0로 대체\n","1086 파일을 0로 대체\n","1088 파일을 0로 대체\n","1153 파일을 0로 대체\n","1154 파일을 0로 대체\n","1155 파일을 0로 대체\n","1156 파일을 0로 대체\n","1157 파일을 0로 대체\n","1158 파일을 0로 대체\n","1198 파일을 0로 대체\n","1215 파일을 0로 대체\n","1230 파일을 0로 대체\n","1293 파일을 0로 대체\n","1320 파일을 0로 대체\n","1456 파일을 0로 대체\n","1543 파일을 0로 대체\n","1551 파일을 0로 대체\n","1552 파일을 0로 대체\n","1808 파일을 0로 대체\n"]}]}]}