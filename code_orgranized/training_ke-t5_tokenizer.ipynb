{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMI8EPM2UK9DOIw3TQAJCND"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install xml_to_dict #https://github.com/xthehatterx/xml_to_dict\n","!pip install sentencepiece\n","!pip install konlpy\n","!pip install transformers"],"metadata":{"id":"Mo6tBTg2caIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","import random\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Union"],"metadata":{"id":"JaUU3duQcRb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkUZCQc2btiK"},"outputs":[],"source":["xml_sample_loc = 'xml_sample_20230519.csv'"]},{"cell_type":"code","source":["xml_sample = pd.read_csv(xml_sample_loc)\n","keyword = xml_sample['keyword']"],"metadata":{"id":"iW8OqAEVb6Lo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get 중복 없는 keyword\n","keyword_unique = []\n","\n","for i in range(len(keyword)):\n","  keyword_unique+=keyword[i].split('|')\n","\n","keyword_unique = list(set(keyword_unique))\n","\n","print(len(keyword_unique) )\n","print(keyword_unique[:20])"],"metadata":{"id":"iuVEFGcMb8wr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#json파일에서 text들을 가져와 text_sum에 저장\n","import json\n","import os\n","\n","text_sum = []\n","\n","folder_path = \"data/\"  # 데이터 폴더 경로\n","\n","# 폴더 내의 파일 목록 가져오기\n","file_list = os.listdir(folder_path)\n","\n","for file_name in file_list:\n","    if file_name.endswith(\".json\"):  # .json 파일인 경우에만 처리\n","        file_path = os.path.join(folder_path, file_name)  # 파일 경로 생성\n","\n","    try:\n","      with open(file_path, \"r\") as file_:\n","          data = json.load(file_)\n","\n","      for i in range(len(data['form'])):\n","          if(data['form'][i]['text']!=None):\n","            text_sum.append(data['form'][i]['text'])  # 요소 추가\n","\n","    except:\n","      print(f\"error in {file_path}\")\n","\n","\n","print(len(text_sum))\n","print(text_sum[0])"],"metadata":{"id":"iQ4Hu1aXcAGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tokenization_data = np.concatenate((text_sum,keyword_unique)) #text와 keyword 합쳐서 한번에 train\n","\n","print(len(train_tokenization_data))\n","\n","def get_training_corpus(): #개수가 많으므로 1000개씩 끊어서 return\n","    return (\n","        train_tokenization_data[i : i + 1000]\n","        for i in range(0, len(train_tokenization_data), 1000)\n","    )\n","\n","training_corpus = get_training_corpus()"],"metadata":{"id":"hkh5A1hqcDF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_tokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR/ke-t5-base-ko\") #pre-trained된거 불러오기\n","old_tokenizer.vocab_size #64100"],"metadata":{"id":"UxvtW66-cEwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 15000) #뒤 숫자는 우리 데이터에서 단어 개수\n","tokenizer.vocab_size #14602\n","tokenizer.save_pretrained(\"tokenizer_finetuned_ket5_by_xml_data\") #저장"],"metadata":{"id":"xPf-ZJh_cGJZ"},"execution_count":null,"outputs":[]}]}