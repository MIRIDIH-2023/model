{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_bert\n",
        "!pip install keras_radam\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgqrvzab4eAY",
        "outputId": "950f2981-1806-4838-e2e7-5cfd676b0869"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_bert\n",
            "  Using cached keras-bert-0.89.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_bert) (1.22.4)\n",
            "Collecting keras-transformer==0.40.0 (from keras_bert)\n",
            "  Using cached keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert)\n",
            "  Using cached keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert)\n",
            "  Using cached keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert)\n",
            "  Using cached keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert)\n",
            "  Using cached keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert)\n",
            "  Using cached keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert)\n",
            "  Using cached keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33501 sha256=d31c36f8b47dc52f39455d1812f7330564bf21eddbae8c6a15129e6bb4f4b9cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/0c/04/646b6fdf6375911b42c8d540a8a3fda8d5d77634e5dcbe7b26\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12287 sha256=b59917a6ccb12bcbe73cf3b135e03015d0b3d2f364e1dfa474358c409f3d9d11\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/cb/22/75a0ad376129177f7c95c0d91331a18f5368fd657f4035ba7c\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3943 sha256=c622d9ca5143fb3d3e444bffccbe907342b99165fbb7a9d68df45f534b523dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/32/c7/fd35d0d1b840a6c7cbd4343f808d10d0f7b87d271a4dbe796f\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4653 sha256=6665096bb901a495cf6de2c88995099f8c6e3f1b41a47c91ef1332707676457b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/3a/4b/21db23c0cc56c4b219616e181f258eb7c57d36cc5d056fae9a\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14979 sha256=34dff0b0634117abbf870ae7cf25860a3f6314c2bacb6b9b44bdbec7f58143b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6946 sha256=bfa3a9cc5620ba6a7847bc353afc8b454995f27d797d502217c3125384b39f43\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/07/1b/b1ca47b6ac338554b75c8f52c54e6a2bfbe1b07d79579979a4\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4968 sha256=8d50024876cec8e5028cec207669b6590f13b7df56fa96d7603e2c66c823f8ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/6a/04/d1706a53b23b2cb5f9a0a76269bf87925daa1bca09eac01b21\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=16f99260aaeed6d30702d86ac6828846307f97796c19b5c73bc4b543a0a3fda1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "Successfully built keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, keras-multi-head, keras-transformer, keras_bert\n",
            "Successfully installed keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_radam in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_radam) (1.22.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.10/dist-packages (from keras_radam) (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tvfw_jhk4WeJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import keras as keras\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras import Input, Model\n",
        "from keras import optimizers\n",
        "\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
        "from keras_bert import Tokenizer\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "\n",
        "from keras_radam import RAdam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lXHpymqm49Zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a053fc6-9f51-4415-b969-9cef743fc791"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LIvOnwyE4WeM"
      },
      "outputs": [],
      "source": [
        "vocab_path = '/content/drive/MyDrive/model/bert/vocab.txt'\n",
        "config_path = '/content/drive/MyDrive/model/bert/bert_config.json'\n",
        "checkpoint_path = '/content/drive/MyDrive/model/bert/bert_model.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l3CKth-d4WeN"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/data/xml/xml.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/xml')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_files = glob.glob('data/xml/data/processed_sample_*.json')\n",
        "random.shuffle(json_files)\n",
        "\n",
        "SEQ_LEN = 512\n",
        "\n",
        "text_data = []\n",
        "\n",
        "for json_file in json_files:\n",
        "    with open(json_file, 'r') as file:\n",
        "        data_dict = json.load(file)\n",
        "        text_data.append(\"\")\n",
        "        for text_info in data_dict['form']:\n",
        "            text = text_info['text']\n",
        "            if type(text) == str:\n",
        "                text_data[-1] += text + ' '"
      ],
      "metadata": {
        "id": "eCa8lqMa9Qjv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hYZSwUoT4WeN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "69f92039-fe84-4f24-fa6b-39b6c9fc5070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introduction Enter the subtitle 01 enter the contents The Name\\nOf The Rose\\n(1986) Cinema and literature You can enter general information describing the page. Double-click the text box to enter the information. Please enter a brief summary about the topic here. Use the right page layout to suit your purposes. You can enter general information describing the page.\\xa0\\nDouble-click the text box to enter the information Please enter a brief summary about the topic here. Use the right page layout to suit your purposes. You can enter general information describing the page. Double-click the text box to enter the information. Source: Enter the source information. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text_data[130]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fqWsTwOK4WeO"
      },
      "outputs": [],
      "source": [
        "class inherit_Tokenizer(Tokenizer):\n",
        "  def _tokenize(self, text):\n",
        "        if not self._cased:\n",
        "            text = text\n",
        "\n",
        "            text = text.lower()\n",
        "        spaced = ''\n",
        "        for ch in text:\n",
        "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
        "                spaced += ' ' + ch + ' '\n",
        "            elif self._is_space(ch):\n",
        "                spaced += ' '\n",
        "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
        "                continue\n",
        "            else:\n",
        "                spaced += ch\n",
        "        tokens = []\n",
        "        for word in spaced.strip().split():\n",
        "            tokens += self._word_piece_tokenize(word)\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9Qs7_a5o4WeO"
      },
      "outputs": [],
      "source": [
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        if \"_\" in token:\n",
        "          token = token.replace(\"_\",\"\")\n",
        "          token = \"##\" + token\n",
        "        token_dict[token] = len(token_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IwL7HVM04WeO"
      },
      "outputs": [],
      "source": [
        "tokenizer = inherit_Tokenizer(token_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qZXdgy-x4WeP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata = pd.read_csv('/content/drive/MyDrive/data/metadata/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D7N7COFV4WeP"
      },
      "outputs": [],
      "source": [
        "keyword_dict = {}\n",
        "\n",
        "for i in range(len(metadata)):\n",
        "    keyword = metadata['keyword'][i]\n",
        "    if type(keyword) == str:\n",
        "        keyword = keyword.split('|')\n",
        "        for word in keyword:\n",
        "            if word not in keyword_dict:\n",
        "                keyword_dict[word] = len(keyword_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SfwIbbL_4WeP"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_keyword(keyword):\n",
        "    one_hot_encoding = np.zeros(len(keyword_dict))\n",
        "    if type(keyword) == str:\n",
        "        keyword = keyword.split('|')\n",
        "        for word in keyword:\n",
        "            one_hot_encoding[keyword_dict[word]] = 1\n",
        "    return one_hot_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_A9PMvLM4WeP"
      },
      "outputs": [],
      "source": [
        "def convert_data():\n",
        "    global tokenizer\n",
        "    indices, targets = [], []\n",
        "    for i in tqdm(range(len(text_data))):\n",
        "        ids, segments = tokenizer.encode(text_data[i], max_len=SEQ_LEN)\n",
        "        indices.append(ids)\n",
        "        targets.append(get_one_hot_keyword(metadata['keyword'][i]))\n",
        "    items = list(zip(indices, targets))\n",
        "\n",
        "    indices, targets = zip(*items)\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)], np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Gr57yaDj4WeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d995a8cf-e425-4118-adef-6108041ba955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3013/3013 [00:01<00:00, 1874.79it/s]\n"
          ]
        }
      ],
      "source": [
        "data_x, data_y = convert_data()\n",
        "train_x, train_y = [data_x[0][:3000], data_x[1][:3000]], data_y[:3000]\n",
        "test_x, test_y = [data_x[0][3000:], data_x[1][3000:]], data_y[3000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "qYa40vbl4WeQ"
      },
      "outputs": [],
      "source": [
        "layer_num = 12\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training=True,\n",
        "    trainable=True,\n",
        "    seq_len=SEQ_LEN,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r-o21aVB4WeQ"
      },
      "outputs": [],
      "source": [
        "CLASS_NUM = len(keyword_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "\n",
        "    return f1_val\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    numerator = 2 * K.sum(y_true * y_pred, axis=-1)\n",
        "    denominator = K.sum(y_true + y_pred, axis=-1)\n",
        "\n",
        "    return 1 - (numerator + 1) / (denominator + 1)\n"
      ],
      "metadata": {
        "id": "mEMO_Dl1UR8D"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "LPkZfdGJ4WeQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "def get_bert_multilabel_model(model):\n",
        "    inputs = model.inputs[:2]\n",
        "    dense = model.layers[-3].output\n",
        "\n",
        "    hidden = keras.layers.Dense(1024, activation='relu')(dense)\n",
        "\n",
        "    outputs = keras.layers.Dense(CLASS_NUM, activation='sigmoid',\n",
        "                                 kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                                 name = 'real_output')(hidden)\n",
        "\n",
        "    multilabel_model = keras.models.Model(inputs, outputs)\n",
        "    multilabel_model.compile(\n",
        "        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=0.00001, weight_decay=0.0025),\n",
        "        loss=dice_loss,\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), f1_score]\n",
        "    )\n",
        "\n",
        "    return multilabel_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "dRavwkBf4WeQ"
      },
      "outputs": [],
      "source": [
        "multilabel_model = get_bert_multilabel_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "WlfTxtxz4WeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358d05c6-cfdc-4670-9980-5d7177823a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# GPU 장치 목록을 가져옵니다.\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "  # GPU가 있는 경우 해당 정보를 출력합니다.\n",
        "  for i, gpu in enumerate(gpus):\n",
        "    print(f'GPU {i}: {gpu}')\n",
        "else:\n",
        "  print('No GPUs detected.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "XtInvTAC4WeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fc5ca9-0a3a-476d-d91d-95acd83d912c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "188/188 [==============================] - 94s 327ms/step - loss: 0.9896 - binary_accuracy: 0.5241 - f1_score: 0.0111 - val_loss: 0.9873 - val_binary_accuracy: 0.5726 - val_f1_score: 0.0151\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.9876 - binary_accuracy: 0.6481 - f1_score: 0.0184 - val_loss: 0.9826 - val_binary_accuracy: 0.7319 - val_f1_score: 0.0306\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 57s 303ms/step - loss: 0.9809 - binary_accuracy: 0.7985 - f1_score: 0.0337 - val_loss: 0.9665 - val_binary_accuracy: 0.8592 - val_f1_score: 0.0544\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.9566 - binary_accuracy: 0.8999 - f1_score: 0.0596 - val_loss: 0.9158 - val_binary_accuracy: 0.9351 - val_f1_score: 0.1052\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 57s 303ms/step - loss: 0.9154 - binary_accuracy: 0.9509 - f1_score: 0.0937 - val_loss: 0.8565 - val_binary_accuracy: 0.9654 - val_f1_score: 0.1431\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.8781 - binary_accuracy: 0.9730 - f1_score: 0.1283 - val_loss: 0.7998 - val_binary_accuracy: 0.9785 - val_f1_score: 0.2123\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.8568 - binary_accuracy: 0.9795 - f1_score: 0.1456 - val_loss: 0.7660 - val_binary_accuracy: 0.9832 - val_f1_score: 0.2364\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.8447 - binary_accuracy: 0.9830 - f1_score: 0.1568 - val_loss: 0.7487 - val_binary_accuracy: 0.9849 - val_f1_score: 0.2559\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 56s 300ms/step - loss: 0.8399 - binary_accuracy: 0.9841 - f1_score: 0.1597 - val_loss: 0.7557 - val_binary_accuracy: 0.9860 - val_f1_score: 0.2473\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 56s 300ms/step - loss: 0.8369 - binary_accuracy: 0.9849 - f1_score: 0.1621 - val_loss: 0.7516 - val_binary_accuracy: 0.9862 - val_f1_score: 0.2505\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.8355 - binary_accuracy: 0.9853 - f1_score: 0.1637 - val_loss: 0.7444 - val_binary_accuracy: 0.9865 - val_f1_score: 0.2538\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 57s 302ms/step - loss: 0.8336 - binary_accuracy: 0.9861 - f1_score: 0.1653 - val_loss: 0.7332 - val_binary_accuracy: 0.9877 - val_f1_score: 0.2717\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 56s 300ms/step - loss: 0.8328 - binary_accuracy: 0.9863 - f1_score: 0.1659 - val_loss: 0.7368 - val_binary_accuracy: 0.9877 - val_f1_score: 0.2717\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 56s 300ms/step - loss: 0.8317 - binary_accuracy: 0.9867 - f1_score: 0.1669 - val_loss: 0.7409 - val_binary_accuracy: 0.9878 - val_f1_score: 0.2617\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.8314 - binary_accuracy: 0.9868 - f1_score: 0.1670Restoring model weights from the end of the best epoch: 12.\n",
            "188/188 [==============================] - 57s 301ms/step - loss: 0.8314 - binary_accuracy: 0.9868 - f1_score: 0.1670 - val_loss: 0.7379 - val_binary_accuracy: 0.9881 - val_f1_score: 0.2656\n",
            "Epoch 15: early stopping\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "history = multilabel_model.fit(\n",
        "    train_x, train_y,\n",
        "    epochs=100,\n",
        "    batch_size=16,\n",
        "    verbose = 1,\n",
        "    validation_data=(test_x, test_y),\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Z57eiadl4WeR"
      },
      "outputs": [],
      "source": [
        "def predict_keywords(input_text, threshold=0.5):\n",
        "    ids, _ = tokenizer.encode(input_text, max_len=SEQ_LEN)\n",
        "    in_data = [tf.expand_dims(ids, 0), tf.expand_dims(np.zeros_like(ids), 0)]\n",
        "\n",
        "    predictions = multilabel_model.predict(in_data)\n",
        "    print(max(predictions[0]))\n",
        "\n",
        "    keywords = [k for k, v in keyword_dict.items() if predictions[0][v] >= threshold]\n",
        "\n",
        "    return keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "i = random.randint(1, 3000)\n",
        "pred = predict_keywords(text_data[i])\n",
        "print(pred)\n",
        "keyword = metadata['keyword'][i]\n",
        "if type(keyword) == str:\n",
        "    keyword = keyword.split('|')\n",
        "print(keyword)\n",
        "\n",
        "print(set(pred) & set(keyword))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcB0vp5aJ869",
        "outputId": "0ae79877-7c82-42bb-cec7-3d074342919a"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "0.9999987\n",
            "['심플', '안내', '사진', '화이트', '강의', '과제', '교육', '학교', '학생', '깔끔', '심플한', '프레임', '이벤트', '디자인', '일러스트', '회사', '귀여운', '표', '홍보', '대학생', '마케팅', '라인', '비즈니스', '소개', '보고서', '도형', '모던', '제안서', '강조', '기업', '아이콘', '카드뉴스', '그래프', '발표', '피피티', '프레젠테이션', 'PPT', '파워포인트', '프리젠테이션', '조별과제', 'ppt', '기획서', '포트폴리오', '사업', '목록', '분할', '플랫한', '상세페이지']\n",
            "['발표', '안내', '직장', '패션', '회사', '대학생', '보고서', '프로젝트', '프레젠테이션', 'ppt', 'PPT', '발표', '안내', '직장', '패션', '회사', '대학생', '보고서', '프로젝트', '프레젠테이션', 'ppt', 'PPT', '발표', '안내', '직장', '패션', '회사', '대학생', '보고서', '프로젝트', '프레젠테이션', 'ppt', 'PPT', '발표', '안내', '직장', '패션', '회사', '대학생', '보고서', '프로젝트', '프레젠테이션', 'ppt', 'PPT', '발표', '안내', '직장', '패션', '회사', '대학생', '보고서', '프로젝트', '프레젠테이션', 'ppt', 'PPT', '발표', '안내', '직장', '패션', '회사', '대학생', '보고서', '프로젝트', '프레젠테이션', 'ppt', 'PPT']\n",
            "{'발표', '보고서', '프레젠테이션', '대학생', 'ppt', '회사', 'PPT', '안내'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DA3GkTvJ_6F",
        "outputId": "9aa78631-559c-409a-d199-350044acabba"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzAhKFg7LNFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}