{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P-YkT_FbqH1n"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('processed_sample.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/text')\n",
        "\n",
        "json_files = glob.glob('data/text/processed_sample_*.json')\n",
        "\n",
        "jsons = []\n",
        "samples = []\n",
        "\n",
        "for json_file in json_files:\n",
        "    with open(json_file, 'r') as file:\n",
        "        data_dict = json.load(file)\n",
        "        jsons.append(data_dict)\n",
        "        samples.append(\"\")\n",
        "        for text_info in data_dict['form']:\n",
        "            text = text_info['text']\n",
        "            if type(text) == str:\n",
        "                samples[-1] += text + ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AudRAXJwqH1p"
      },
      "outputs": [],
      "source": [
        "keyword_dict = {}\n",
        "\n",
        "for i in range(len(jsons)):\n",
        "    keyword = jsons[i]['keyword']\n",
        "    for word in keyword:\n",
        "        if word not in keyword_dict:\n",
        "            keyword_dict[word] = len(keyword_dict)\n",
        "\n",
        "keyword_num = len(keyword_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3gTBIAkiqH1q"
      },
      "outputs": [],
      "source": [
        "def get_one_hot(keywords):\n",
        "    one_hot = torch.zeros(keyword_num)\n",
        "    for keyword in keywords:\n",
        "        one_hot[keyword_dict[keyword]] = 1\n",
        "    return one_hot\n",
        "\n",
        "def get_targets():\n",
        "    targets = []\n",
        "    for json in jsons:\n",
        "        targets.append(get_one_hot(json['keyword']))\n",
        "    return torch.stack(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WEbS4ZgBqH1q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "targets = get_targets()\n",
        "data = list(zip(samples, targets))\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "train_samples = [x[0] for x in data[:int(len(data) * 0.8)]]\n",
        "train_targets = torch.stack([x[1] for x in data[:int(len(data) * 0.8)]])\n",
        "\n",
        "test_samples = [x[0] for x in data[int(len(data) * 0.8):]]\n",
        "test_targets = torch.stack([x[1] for x in data[int(len(data) * 0.8):]])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sYCAX9xqs5N",
        "outputId": "4bc032a7-c6fe-43e8-d67e-2c1bb7b15b4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.30.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.6.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zCKKu7fBqH1s"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "from torch import nn\n",
        "\n",
        "model_name = 'distiluse-base-multilingual-cased-v1'\n",
        "\n",
        "sbert = SentenceTransformer(model_name)\n",
        "sbert.max_seq_length = 512\n",
        "dense1 = models.Dense(in_features=512, out_features=1024, activation_function=nn.ReLU())\n",
        "dense2 = models.Dense(in_features=1024, out_features=1024, activation_function=nn.ReLU())\n",
        "dense3 = models.Dense(in_features=1024, out_features=keyword_num, activation_function=nn.Sigmoid())\n",
        "\n",
        "model = SentenceTransformer(modules=[sbert, dense1, dense2, dense3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class StringTensorDataset(Dataset):\n",
        "    def __init__(self, samples, targets):\n",
        "        self.samples = samples\n",
        "        self.targets = targets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.samples[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "iD1RHh2NyIQP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sentence_transformers.util import batch_to_device\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "train_targets = train_targets.to(device)\n",
        "test_targets = test_targets.to(device)\n",
        "\n",
        "train_dataset = StringTensorDataset(train_samples, train_targets)\n",
        "test_dataset = StringTensorDataset(test_samples, test_targets)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = FocalLoss(alpha=0.5, gamma=4)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds, train_actuals = [], []\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch_to_device(model.tokenize(inputs), device))['sentence_embedding']\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.requires_grad_(True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        train_preds += (outputs.detach().cpu().numpy() > 0.5).tolist()\n",
        "        train_actuals += targets.detach().cpu().numpy().tolist()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    train_f1_score = f1_score(train_actuals, train_preds, average='micro')\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    eval_preds, eval_actuals = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "            outputs = model(batch_to_device(model.tokenize(inputs), device))['sentence_embedding']\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            eval_preds += (outputs.detach().cpu().numpy() > 0.5).tolist()\n",
        "            eval_actuals += targets.detach().cpu().numpy().tolist()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(test_dataloader)\n",
        "    eval_f1_score = f1_score(eval_actuals, eval_preds, average='micro')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train F1 Score: {train_f1_score:.4f}\")\n",
        "    print(f\"Eval Loss: {avg_eval_loss:.4f}, Eval F1 Score: {eval_f1_score:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "ETTjokEIu01l",
        "outputId": "dd637636-ac50-4be5-9916-f86f5ec5375d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 76/76 [00:14<00:00,  5.08it/s]\n",
            "Evaluating: 100%|██████████| 19/19 [00:01<00:00, 15.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.0248, Train F1 Score: 0.0066\n",
            "Eval Loss: 0.0217, Eval F1 Score: 0.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 76/76 [00:14<00:00,  5.31it/s]\n",
            "Evaluating: 100%|██████████| 19/19 [00:01<00:00, 15.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 0.0217, Train F1 Score: 0.0000\n",
            "Eval Loss: 0.0217, Eval F1 Score: 0.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 76/76 [00:14<00:00,  5.23it/s]\n",
            "Evaluating: 100%|██████████| 19/19 [00:01<00:00, 15.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10\n",
            "Train Loss: 0.0217, Train F1 Score: 0.0000\n",
            "Eval Loss: 0.0217, Eval F1 Score: 0.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 76/76 [00:14<00:00,  5.22it/s]\n",
            "Evaluating: 100%|██████████| 19/19 [00:01<00:00, 15.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10\n",
            "Train Loss: 0.0217, Train F1 Score: 0.0000\n",
            "Eval Loss: 0.0217, Eval F1 Score: 0.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 76/76 [00:15<00:00,  5.05it/s]\n",
            "Evaluating: 100%|██████████| 19/19 [00:01<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10\n",
            "Train Loss: 0.0217, Train F1 Score: 0.0000\n",
            "Eval Loss: 0.0217, Eval F1 Score: 0.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▎       | 18/76 [00:03<00:12,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-af8afad85b2f>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_preds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_actuals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_w5EYdUJHagD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}